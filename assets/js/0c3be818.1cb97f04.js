"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[755],{4610(r,n,e){e.r(n),e.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"module-3/sim-to-real","title":"Sim-to-Real Transfer","description":"Comprehensive guide to transferring AI models and behaviors from simulation to real humanoid robots","source":"@site/docs/module-3/sim-to-real.md","sourceDirName":"module-3","slug":"/module-3/sim-to-real","permalink":"/physical-ai/docs/module-3/sim-to-real","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-textbook/book-ai/tree/main/docs/module-3/sim-to-real.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"id":"sim-to-real","title":"Sim-to-Real Transfer","sidebar_position":7,"description":"Comprehensive guide to transferring AI models and behaviors from simulation to real humanoid robots","keywords":["sim-to-real","transfer learning","robotics simulation","domain adaptation","Isaac Sim","reality gap"]},"sidebar":"textbook","previous":{"title":"Nav2 Path Planning","permalink":"/physical-ai/docs/module-3/nav2-path-planning"}}');var t=e(4848),i=e(8453);const s={id:"sim-to-real",title:"Sim-to-Real Transfer",sidebar_position:7,description:"Comprehensive guide to transferring AI models and behaviors from simulation to real humanoid robots",keywords:["sim-to-real","transfer learning","robotics simulation","domain adaptation","Isaac Sim","reality gap"]},o="Sim-to-Real Transfer: Bridging Digital and Physical AI",l={},m=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Sim-to-Real Transfer",id:"introduction-to-sim-to-real-transfer",level:2},{value:"The Reality Gap Problem",id:"the-reality-gap-problem",level:3},{value:"Sim-to-Real Transfer Approaches",id:"sim-to-real-transfer-approaches",level:3},{value:"Key Challenges in Sim-to-Real Transfer",id:"key-challenges-in-sim-to-real-transfer",level:3},{value:"Domain Randomization Techniques",id:"domain-randomization-techniques",level:2},{value:"Understanding Domain Randomization",id:"understanding-domain-randomization",level:3},{value:"Advanced Domain Randomization",id:"advanced-domain-randomization",level:3},{value:"Physics Simulation Accuracy",id:"physics-simulation-accuracy",level:2},{value:"Accurate Physics Modeling",id:"accurate-physics-modeling",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2},{value:"High-Fidelity Synthetic Data",id:"high-fidelity-synthetic-data",level:3},{value:"Sensor Simulation and Calibration",id:"sensor-simulation-and-calibration",level:2},{value:"Realistic Sensor Simulation",id:"realistic-sensor-simulation",level:3},{value:"Transfer Learning Techniques",id:"transfer-learning-techniques",level:2},{value:"Domain Adaptation Methods",id:"domain-adaptation-methods",level:3},{value:"Validation and Testing",id:"validation-and-testing",level:2},{value:"Transfer Validation Framework",id:"transfer-validation-framework",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Adaptive Parameter Tuning",id:"adaptive-parameter-tuning",level:3}];function _(r){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...r.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"sim-to-real-transfer-bridging-digital-and-physical-ai",children:"Sim-to-Real Transfer: Bridging Digital and Physical AI"})}),"\n",(0,t.jsx)(n.p,{children:'Sim-to-real transfer is a critical capability that enables the deployment of AI models and behaviors developed in simulation onto real humanoid robots. This chapter covers techniques to minimize the "reality gap" and ensure successful transfer of learned behaviors from virtual to physical environments.'}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the challenges and solutions in sim-to-real transfer"}),"\n",(0,t.jsx)(n.li,{children:"Implement domain randomization techniques for robust simulation"}),"\n",(0,t.jsx)(n.li,{children:"Apply domain adaptation methods for transfer learning"}),"\n",(0,t.jsx)(n.li,{children:"Create realistic simulation environments that match real conditions"}),"\n",(0,t.jsx)(n.li,{children:"Validate and verify transferred behaviors on physical robots"}),"\n",(0,t.jsx)(n.li,{children:"Optimize simulation parameters for better transfer performance"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-sim-to-real-transfer",children:"Introduction to Sim-to-Real Transfer"}),"\n",(0,t.jsx)(n.h3,{id:"the-reality-gap-problem",children:"The Reality Gap Problem"}),"\n",(0,t.jsx)(n.p,{children:'The "reality gap" refers to the discrepancy between simulation and real-world performance. This gap arises from several factors:'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Imperfections"}),": Inaccuracies in robot dynamics, kinematics, and sensor models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environmental Differences"}),": Lighting, textures, friction coefficients, and surface properties"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Noise"}),": Differences in sensor characteristics, noise patterns, and latencies"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Actuator Dynamics"}),": Motor responses, gear backlash, and mechanical compliance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physical Phenomena"}),": Air resistance, electromagnetic interference, and thermal effects"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"sim-to-real-transfer-approaches",children:"Sim-to-Real Transfer Approaches"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                         Sim-to-Real Pipeline                          \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\r\n\u2502  \u2502   Physical  \u2502    \u2502   Domain         \u2502    \u2502   Transfer          \u2502    \u2502\r\n\u2502  \u2502   System    \u2502\u25c4\u2500\u2500\u2500\u2524   Randomization  \u2502\u25c4\u2500\u2500\u2500\u2524   Learning &        \u2502    \u2502\r\n\u2502  \u2502   Modeling  \u2502    \u2502   & Adaptation   \u2502    \u2502   Validation        \u2502    \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\r\n\u2502         \u2502                       \u2502                       \u2502              \u2502\r\n\u2502         \u25bc                       \u25bc                       \u25bc              \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\r\n\u2502  \u2502   Reality   \u2502    \u2502   Synthetic      \u2502    \u2502   Real Robot        \u2502    \u2502\r\n\u2502  \u2502   Gap       \u2502    \u2502   Data &         \u2502    \u2502   Deployment        \u2502    \u2502\r\n\u2502  \u2502   Analysis  \u2502    \u2502   Domain Shift   \u2502    \u2502   & Testing         \u2502    \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"key-challenges-in-sim-to-real-transfer",children:"Key Challenges in Sim-to-Real Transfer"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Domain Shift"}),": Differences in lighting, textures, colors, and camera characteristics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physical Property Differences"}),": Friction, mass, inertia, and compliance variations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Noise Characteristics"}),": Different noise patterns between simulated and real sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temporal Dynamics"}),": Timing differences in actuator responses and sensor updates"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environmental Factors"}),": Temperature, humidity, and atmospheric conditions"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"domain-randomization-techniques",children:"Domain Randomization Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"understanding-domain-randomization",children:"Understanding Domain Randomization"}),"\n",(0,t.jsx)(n.p,{children:"Domain randomization is a technique that increases the diversity of training data by varying environmental properties randomly during simulation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nimport random\r\nfrom dataclasses import dataclass\r\nfrom typing import Dict, Tuple, List\r\n\r\n@dataclass\r\nclass DomainRandomizationConfig:\r\n    \"\"\"Configuration for domain randomization parameters\"\"\"\r\n    lighting: Dict[str, Tuple[float, float]] = None\r\n    materials: Dict[str, Tuple[float, float]] = None\r\n    objects: Dict[str, Tuple[float, float]] = None\r\n    camera: Dict[str, Tuple[float, float]] = None\r\n    environment: Dict[str, Tuple[float, float]] = None\r\n    physics: Dict[str, Tuple[float, float]] = None\r\n\r\nclass DomainRandomizer:\r\n    def __init__(self, config: DomainRandomizationConfig):\r\n        self.config = config\r\n        self.episode_count = 0\r\n\r\n    def randomize_lighting(self):\r\n        \"\"\"Randomize lighting conditions in the simulation\"\"\"\r\n        lighting_config = self.config.lighting\r\n        if not lighting_config:\r\n            return {\r\n                'intensity': 3000,\r\n                'color': [1.0, 1.0, 1.0],\r\n                'position': [5.0, 5.0, 10.0],\r\n                'direction': [-0.5, -0.5, -1.0]\r\n            }\r\n\r\n        # Randomize lighting parameters\r\n        intensity = random.uniform(*lighting_config.get('intensity', (1000, 5000)))\r\n        color = [\r\n            random.uniform(*lighting_config.get('color_range', (0.5, 1.0))) for _ in range(3)\r\n        ]\r\n        position = [\r\n            random.uniform(*lighting_config.get('position_range', (-10, 10))) for _ in range(3)\r\n        ]\r\n        direction = [\r\n            random.uniform(*lighting_config.get('direction_range', (-1, 1))) for _ in range(3)\r\n        ]\r\n\r\n        return {\r\n            'intensity': intensity,\r\n            'color': color,\r\n            'position': position,\r\n            'direction': direction\r\n        }\r\n\r\n    def randomize_materials(self):\r\n        \"\"\"Randomize material properties\"\"\"\r\n        materials_config = self.config.materials\r\n        if not materials_config:\r\n            return {\r\n                'roughness': 0.5,\r\n                'metallic': 0.0,\r\n                'specular': 0.5,\r\n                'albedo': [0.8, 0.8, 0.8]\r\n            }\r\n\r\n        roughness = random.uniform(*materials_config.get('roughness_range', (0.0, 1.0)))\r\n        metallic = random.uniform(*materials_config.get('metallic_range', (0.0, 1.0)))\r\n        specular = random.uniform(*materials_config.get('specular_range', (0.0, 1.0)))\r\n        albedo = [\r\n            random.uniform(*materials_config.get('albedo_range', (0.1, 1.0))) for _ in range(3)\r\n        ]\r\n\r\n        return {\r\n            'roughness': roughness,\r\n            'metallic': metallic,\r\n            'specular': specular,\r\n            'albedo': albedo\r\n        }\r\n\r\n    def randomize_physics_properties(self):\r\n        \"\"\"Randomize physics properties to account for real-world uncertainties\"\"\"\r\n        physics_config = self.config.physics\r\n        if not physics_config:\r\n            return {\r\n                'gravity': [0, 0, -9.81],\r\n                'friction': 0.5,\r\n                'restitution': 0.1,\r\n                'mass_multiplier': 1.0\r\n            }\r\n\r\n        gravity = [\r\n            random.uniform(*physics_config.get('gravity_range', (-0.1, 0.1))),\r\n            random.uniform(*physics_config.get('gravity_range', (-0.1, 0.1))),\r\n            random.uniform(*physics_config.get('gravity_range', (-9.9, -9.7)))\r\n        ]\r\n\r\n        friction = random.uniform(*physics_config.get('friction_range', (0.1, 1.0)))\r\n        restitution = random.uniform(*physics_config.get('restitution_range', (0.0, 0.5)))\r\n        mass_multiplier = random.uniform(*physics_config.get('mass_range', (0.8, 1.2)))\r\n\r\n        return {\r\n            'gravity': gravity,\r\n            'friction': friction,\r\n            'restitution': restitution,\r\n            'mass_multiplier': mass_multiplier\r\n        }\r\n\r\n    def randomize_camera_parameters(self):\r\n        \"\"\"Randomize camera parameters to simulate sensor variations\"\"\"\r\n        camera_config = self.config.camera\r\n        if not camera_config:\r\n            return {\r\n                'focal_length': 24.0,\r\n                'sensor_width': 36.0,\r\n                'sensor_height': 24.0,\r\n                'distortion_coefficients': [0, 0, 0, 0, 0]\r\n            }\r\n\r\n        focal_length = random.uniform(*camera_config.get('focal_range', (18.0, 55.0)))\r\n        sensor_width = random.uniform(*camera_config.get('sensor_width_range', (35.0, 37.0)))\r\n        sensor_height = random.uniform(*camera_config.get('sensor_height_range', (23.0, 25.0)))\r\n\r\n        # Simulate lens distortion\r\n        distortion_coeffs = [\r\n            random.uniform(*camera_config.get('distortion_range', (-0.1, 0.1))) for _ in range(5)\r\n        ]\r\n\r\n        return {\r\n            'focal_length': focal_length,\r\n            'sensor_width': sensor_width,\r\n            'sensor_height': sensor_height,\r\n            'distortion_coefficients': distortion_coeffs\r\n        }\r\n\r\n    def apply_randomization(self):\r\n        \"\"\"Apply all domain randomization effects\"\"\"\r\n        lighting_params = self.randomize_lighting()\r\n        material_params = self.randomize_materials()\r\n        physics_params = self.randomize_physics_properties()\r\n        camera_params = self.randomize_camera_parameters()\r\n\r\n        self.episode_count += 1\r\n\r\n        return {\r\n            'lighting': lighting_params,\r\n            'materials': material_params,\r\n            'physics': physics_params,\r\n            'camera': camera_params\r\n        }\r\n\r\ndef setup_domain_randomization():\r\n    \"\"\"Setup domain randomization configuration for Isaac Sim\"\"\"\r\n\r\n    config = DomainRandomizationConfig(\r\n        lighting={\r\n            'intensity_range': (1000, 8000),\r\n            'color_range': (0.3, 1.0),\r\n            'position_range': (-15.0, 15.0),\r\n            'direction_range': (-1.0, 1.0)\r\n        },\r\n        materials={\r\n            'roughness_range': (0.0, 1.0),\r\n            'metallic_range': (0.0, 1.0),\r\n            'specular_range': (0.0, 1.0),\r\n            'albedo_range': (0.1, 1.0)\r\n        },\r\n        physics={\r\n            'gravity_range': (-0.2, 0.2),\r\n            'friction_range': (0.1, 1.5),\r\n            'restitution_range': (0.0, 0.8),\r\n            'mass_range': (0.7, 1.3)\r\n        },\r\n        camera={\r\n            'focal_range': (18.0, 85.0),\r\n            'sensor_width_range': (34.0, 38.0),\r\n            'sensor_height_range': (22.0, 26.0),\r\n            'distortion_range': (-0.3, 0.3)\r\n        }\r\n    )\r\n\r\n    randomizer = DomainRandomizer(config)\r\n    return randomizer\n"})}),"\n",(0,t.jsx)(n.h3,{id:"advanced-domain-randomization",children:"Advanced Domain Randomization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nclass DomainRandomizationAugmentation(nn.Module):\r\n    """PyTorch module for domain randomization during training"""\r\n\r\n    def __init__(self,\r\n                 brightness_range=(0.8, 1.2),\r\n                 contrast_range=(0.8, 1.2),\r\n                 saturation_range=(0.8, 1.2),\r\n                 hue_range=(-0.1, 0.1),\r\n                 noise_std_range=(0.0, 0.05),\r\n                 blur_kernel_range=(1, 3)):\r\n        super().__init__()\r\n        self.brightness_range = brightness_range\r\n        self.contrast_range = contrast_range\r\n        self.saturation_range = saturation_range\r\n        self.hue_range = hue_range\r\n        self.noise_std_range = noise_std_range\r\n        self.blur_kernel_range = blur_kernel_range\r\n\r\n    def forward(self, images):\r\n        """Apply domain randomization augmentation to batch of images"""\r\n        augmented_images = []\r\n\r\n        for image in images:\r\n            # Randomize brightness\r\n            brightness_factor = torch.empty(1).uniform_(*self.brightness_range).item()\r\n            augmented_image = transforms.functional.adjust_brightness(image, brightness_factor)\r\n\r\n            # Randomize contrast\r\n            contrast_factor = torch.empty(1).uniform_(*self.contrast_range).item()\r\n            augmented_image = transforms.functional.adjust_contrast(augmented_image, contrast_factor)\r\n\r\n            # Randomize saturation\r\n            saturation_factor = torch.empty(1).uniform_(*self.saturation_range).item()\r\n            augmented_image = transforms.functional.adjust_saturation(augmented_image, saturation_factor)\r\n\r\n            # Randomize hue\r\n            hue_factor = torch.empty(1).uniform_(*self.hue_range).item()\r\n            augmented_image = transforms.functional.adjust_hue(augmented_image, hue_factor)\r\n\r\n            # Add random noise\r\n            noise_std = torch.empty(1).uniform_(*self.noise_std_range).item()\r\n            if noise_std > 0:\r\n                noise = torch.randn_like(augmented_image) * noise_std\r\n                augmented_image = torch.clamp(augmented_image + noise, 0, 1)\r\n\r\n            # Apply random blur\r\n            blur_kernel_size = random.randint(*self.blur_kernel_range)\r\n            if blur_kernel_size > 1:\r\n                # Simple average blur (in practice, use Gaussian blur)\r\n                kernel = torch.ones(1, 1, blur_kernel_size, blur_kernel_size) / (blur_kernel_size ** 2)\r\n                # Apply blur to each channel separately\r\n                blurred_channels = []\r\n                for c in range(augmented_image.shape[0]):\r\n                    channel = augmented_image[c:c+1, :, :].unsqueeze(0)  # Add batch dimension\r\n                    blurred_channel = torch.nn.functional.conv2d(channel, kernel, padding=blur_kernel_size//2)\r\n                    blurred_channels.append(blurred_channel.squeeze(0))\r\n\r\n                augmented_image = torch.cat(blurred_channels, dim=0)\r\n\r\n            augmented_images.append(augmented_image)\r\n\r\n        return torch.stack(augmented_images)\r\n\r\nclass SyntheticToRealDataset(Dataset):\r\n    """Dataset that combines synthetic and real data with domain randomization"""\r\n\r\n    def __init__(self, synthetic_data, real_data, domain_randomization_transform=None):\r\n        self.synthetic_data = synthetic_data\r\n        self.real_data = real_data\r\n        self.domain_randomization = domain_randomization_transform\r\n        self.synth_len = len(synthetic_data)\r\n        self.real_len = len(real_data)\r\n\r\n    def __len__(self):\r\n        return self.synth_len + self.real_len\r\n\r\n    def __getitem__(self, idx):\r\n        if idx < self.synth_len:\r\n            # Synthetic data with domain randomization\r\n            image, label = self.synthetic_data[idx]\r\n            if self.domain_randomization:\r\n                image = self.domain_randomization(image.unsqueeze(0)).squeeze(0)\r\n            domain_label = 0  # Synthetic domain\r\n        else:\r\n            # Real data without randomization\r\n            image, label = self.real_data[idx - self.synth_len]\r\n            domain_label = 1  # Real domain\r\n\r\n        return image, label, domain_label  # Return domain label for domain adaptation\r\n\r\ndef create_domain_adaptation_model(input_dim, hidden_dim, output_dim):\r\n    """Create a model suitable for domain adaptation"""\r\n\r\n    class DomainAdaptationModel(nn.Module):\r\n        def __init__(self, input_dim, hidden_dim, output_dim):\r\n            super().__init__()\r\n\r\n            # Feature extractor (shared between domains)\r\n            self.feature_extractor = nn.Sequential(\r\n                nn.Linear(input_dim, hidden_dim),\r\n                nn.ReLU(),\r\n                nn.Dropout(0.5),\r\n                nn.Linear(hidden_dim, hidden_dim // 2),\r\n                nn.ReLU(),\r\n                nn.Dropout(0.5)\r\n            )\r\n\r\n            # Label classifier (task-specific)\r\n            self.label_classifier = nn.Sequential(\r\n                nn.Linear(hidden_dim // 2, hidden_dim // 4),\r\n                nn.ReLU(),\r\n                nn.Linear(hidden_dim // 4, output_dim)\r\n            )\r\n\r\n            # Domain classifier (for domain adaptation)\r\n            self.domain_classifier = nn.Sequential(\r\n                nn.Linear(hidden_dim // 2, hidden_dim // 4),\r\n                nn.ReLU(),\r\n                nn.Linear(hidden_dim // 4, 1),\r\n                nn.Sigmoid()\r\n            )\r\n\r\n        def forward(self, x, alpha=1.0):\r\n            features = self.feature_extractor(x)\r\n\r\n            # Reverse gradient for domain adaptation\r\n            reversed_features = GradientReversalFunction.apply(features, alpha)\r\n\r\n            label_output = self.label_classifier(features)\r\n            domain_output = self.domain_classifier(reversed_features)\r\n\r\n            return label_output, domain_output\r\n\r\n    class GradientReversalFunction(torch.autograd.Function):\r\n        @staticmethod\r\n        def forward(ctx, input, alpha):\r\n            ctx.alpha = alpha\r\n            return input\r\n\r\n        @staticmethod\r\n        def backward(ctx, grad_output):\r\n            output = grad_output.neg() * ctx.alpha\r\n            return output, None\r\n\r\n    return DomainAdaptationModel(input_dim, hidden_dim, output_dim)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"physics-simulation-accuracy",children:"Physics Simulation Accuracy"}),"\n",(0,t.jsx)(n.h3,{id:"accurate-physics-modeling",children:"Accurate Physics Modeling"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nfrom scipy.spatial.transform import Rotation as R\r\nimport omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.prims import get_prim_at_path\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\r\n\r\nclass PhysicsCalibrator:\r\n    \"\"\"Calibrate physics properties to match real-world behavior\"\"\"\r\n\r\n    def __init__(self, world: World):\r\n        self.world = world\r\n        self.calibration_data = {}\r\n\r\n    def calibrate_robot_dynamics(self, robot_prim_path: str):\r\n        \"\"\"Calibrate robot dynamics properties to match real robot\"\"\"\r\n\r\n        # Get robot articulation\r\n        robot_articulation = self.world.scene.get_object(robot_prim_path.split('/')[-1])\r\n\r\n        # Calibrate joint properties\r\n        joint_names = robot_articulation.dof_names\r\n\r\n        for joint_name in joint_names:\r\n            # Get joint prim\r\n            joint_prim = get_prim_at_path(f\"{robot_prim_path}/{joint_name}\")\r\n\r\n            # Calibrate friction and damping based on real robot data\r\n            self.calibrate_joint_properties(joint_prim)\r\n\r\n        # Calibrate link masses and inertias\r\n        link_paths = self.get_robot_link_paths(robot_prim_path)\r\n        for link_path in link_paths:\r\n            link_prim = get_prim_at_path(link_path)\r\n            self.calibrate_link_properties(link_prim)\r\n\r\n    def calibrate_joint_properties(self, joint_prim):\r\n        \"\"\"Calibrate joint friction and damping\"\"\"\r\n        # Example calibration values - these would come from real robot measurements\r\n        joint_calibration = {\r\n            'friction': 0.1,  # N*m*s\r\n            'damping': 0.05,   # N*m*s/rad\r\n            'stiction': 0.02   # N*m\r\n        }\r\n\r\n        # Apply calibration\r\n        # Note: This is pseudocode - actual Isaac Sim API calls would be different\r\n        # joint_prim.GetAttribute(\"physics:jointFriction\").Set(joint_calibration['friction'])\r\n        # joint_prim.GetAttribute(\"physics:jointDamping\").Set(joint_calibration['damping'])\r\n\r\n    def calibrate_link_properties(self, link_prim):\r\n        \"\"\"Calibrate link mass and inertia properties\"\"\"\r\n        # Get real robot mass properties (from CAD model or measurements)\r\n        real_mass = self.get_real_link_mass(link_prim)\r\n        real_inertia = self.get_real_link_inertia(link_prim)\r\n\r\n        # Update simulation properties\r\n        # link_prim.GetAttribute(\"physics:mass\").Set(real_mass)\r\n        # link_prim.GetAttribute(\"physics:diagonalInertia\").Set(real_inertia)\r\n\r\n    def get_real_link_mass(self, link_prim):\r\n        \"\"\"Get real link mass from calibration data\"\"\"\r\n        link_name = link_prim.GetName()\r\n        if link_name in self.calibration_data:\r\n            return self.calibration_data[link_name].get('mass', 1.0)\r\n        return 1.0  # Default mass\r\n\r\n    def get_real_link_inertia(self, link_prim):\r\n        \"\"\"Get real link inertia from calibration data\"\"\"\r\n        link_name = link_prim.GetName()\r\n        if link_name in self.calibration_data:\r\n            return self.calibration_data[link_name].get('inertia', [1.0, 1.0, 1.0])\r\n        return [1.0, 1.0, 1.0]  # Default inertia\r\n\r\n    def calibrate_surface_properties(self):\r\n        \"\"\"Calibrate surface properties (friction, restitution)\"\"\"\r\n        # Get common surface materials\r\n        surfaces = [\r\n            'floor', 'table', 'walls', 'objects'\r\n        ]\r\n\r\n        for surface in surfaces:\r\n            # Calibrate based on real-world measurements\r\n            surface_properties = self.measure_real_surface_properties(surface)\r\n            self.apply_surface_calibration(surface, surface_properties)\r\n\r\n    def measure_real_surface_properties(self, surface_name):\r\n        \"\"\"Measure real-world surface properties\"\"\"\r\n        # This would involve physical experiments\r\n        # For now, return example values\r\n        surface_properties = {\r\n            'floor': {\r\n                'static_friction': 0.7,\r\n                'dynamic_friction': 0.5,\r\n                'restitution': 0.1\r\n            },\r\n            'table': {\r\n                'static_friction': 0.6,\r\n                'dynamic_friction': 0.4,\r\n                'restitution': 0.05\r\n            },\r\n            'walls': {\r\n                'static_friction': 0.8,\r\n                'dynamic_friction': 0.6,\r\n                'restitution': 0.05\r\n            },\r\n            'objects': {\r\n                'static_friction': 0.5,\r\n                'dynamic_friction': 0.3,\r\n                'restitution': 0.2\r\n            }\r\n        }\r\n\r\n        return surface_properties.get(surface_name, {\r\n            'static_friction': 0.5,\r\n            'dynamic_friction': 0.3,\r\n            'restitution': 0.1\r\n        })\r\n\r\n    def apply_surface_calibration(self, surface_name, properties):\r\n        \"\"\"Apply surface property calibration to simulation\"\"\"\r\n        # Find all prims with this surface material\r\n        # Apply the calibrated properties\r\n        pass\r\n\r\nclass SensorCalibrator:\r\n    \"\"\"Calibrate sensor properties to match real sensors\"\"\"\r\n\r\n    def __init__(self, world: World):\r\n        self.world = world\r\n        self.sensor_specs = {}\r\n\r\n    def calibrate_camera_parameters(self, camera_path: str):\r\n        \"\"\"Calibrate camera intrinsic and extrinsic parameters\"\"\"\r\n        # Get real camera calibration\r\n        real_calib = self.get_real_camera_calibration(camera_path)\r\n\r\n        # Apply to simulation camera\r\n        camera_prim = get_prim_at_path(camera_path)\r\n\r\n        # Update focal length and principal point\r\n        # camera_prim.GetAttribute(\"focalLength\").Set(real_calib['focal_length'])\r\n        # camera_prim.GetAttribute(\"horizontalAperture\").Set(real_calib['aperture_width'])\r\n        # camera_prim.GetAttribute(\"verticalAperture\").Set(real_calib['aperture_height'])\r\n\r\n        # Apply distortion coefficients\r\n        # camera_prim.GetAttribute(\"physics:distortionCoefficientK1\").Set(real_calib['k1'])\r\n        # camera_prim.GetAttribute(\"physics:distortionCoefficientK2\").Set(real_calib['k2'])\r\n        # camera_prim.GetAttribute(\"physics:distortionCoefficientP1\").Set(real_calib['p1'])\r\n        # camera_prim.GetAttribute(\"physics:distortionCoefficientP2\").Set(real_calib['p2'])\r\n\r\n    def get_real_camera_calibration(self, camera_path):\r\n        \"\"\"Get real camera calibration parameters\"\"\"\r\n        # This would load calibration from file or database\r\n        # For now, return example values\r\n        return {\r\n            'focal_length': 24.0,  # mm\r\n            'aperture_width': 36.0,  # mm\r\n            'aperture_height': 24.0,  # mm\r\n            'k1': -0.17187,  # Radial distortion\r\n            'k2': 0.03843,\r\n            'k3': -0.00076,\r\n            'p1': 0.00031,  # Tangential distortion\r\n            'p2': -0.00014\r\n        }\r\n\r\n    def calibrate_imu_properties(self, imu_path: str):\r\n        \"\"\"Calibrate IMU noise properties\"\"\"\r\n        # Get real IMU specifications\r\n        real_spec = self.get_real_imu_specifications(imu_path)\r\n\r\n        # Apply noise parameters to simulation\r\n        # This would involve setting noise parameters on the IMU sensor\r\n        pass\r\n\r\n    def get_real_imu_specifications(self, imu_path):\r\n        \"\"\"Get real IMU specifications\"\"\"\r\n        # Example specifications for common IMUs\r\n        imu_specs = {\r\n            'imu_link': {\r\n                'accelerometer_noise_density': 0.002,  # m/s^2/\u221aHz\r\n                'gyroscope_noise_density': 0.0001,     # rad/s/\u221aHz\r\n                'accelerometer_bias_instability': 0.005,  # m/s^2\r\n                'gyroscope_bias_instability': 0.00001    # rad/s\r\n            }\r\n        }\r\n        return imu_specs.get(imu_path, {\r\n            'accelerometer_noise_density': 0.003,\r\n            'gyroscope_noise_density': 0.0002,\r\n            'accelerometer_bias_instability': 0.008,\r\n            'gyroscope_bias_instability': 0.00002\r\n        })\r\n\r\n    def calibrate_lidar_properties(self, lidar_path: str):\r\n        \"\"\"Calibrate LiDAR properties\"\"\"\r\n        # Get real LiDAR specifications\r\n        real_spec = self.get_real_lidar_specifications(lidar_path)\r\n\r\n        # Apply to simulation LiDAR\r\n        # lidar_prim = get_prim_at_path(lidar_path)\r\n        # lidar_prim.GetAttribute(\"physics:rangeMin\").Set(real_spec['range_min'])\r\n        # lidar_prim.GetAttribute(\"physics:rangeMax\").Set(real_spec['range_max'])\r\n        # lidar_prim.GetAttribute(\"physics:noiseStdDev\").Set(real_spec['noise_std'])\r\n\r\n    def get_real_lidar_specifications(self, lidar_path):\r\n        \"\"\"Get real LiDAR specifications\"\"\"\r\n        lidar_specs = {\r\n            'lidar_sensor': {\r\n                'range_min': 0.1,    # meters\r\n                'range_max': 30.0,   # meters\r\n                'noise_std': 0.01,   # meters\r\n                'angular_resolution': 0.01745,  # radians (1 degree)\r\n                'update_rate': 10.0  # Hz\r\n            }\r\n        }\r\n        return lidar_specs.get(lidar_path, {\r\n            'range_min': 0.2,\r\n            'range_max': 25.0,\r\n            'noise_std': 0.02,\r\n            'angular_resolution': 0.0349,  # 2 degrees\r\n            'update_rate': 5.0\r\n        })\n"})}),"\n",(0,t.jsx)(n.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,t.jsx)(n.h3,{id:"high-fidelity-synthetic-data",children:"High-Fidelity Synthetic Data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.sensor import Camera, LidarRtx\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport json\r\nfrom typing import Dict, List, Tuple\r\nimport os\r\n\r\nclass SyntheticDataManager:\r\n    """Manage synthetic data generation for sim-to-real transfer"""\r\n\r\n    def __init__(self, output_dir: str = "synthetic_data"):\r\n        self.output_dir = output_dir\r\n        self.data_counter = 0\r\n        self.annotation_data = []\r\n\r\n        # Create output directories\r\n        os.makedirs(f"{output_dir}/images", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/depth", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/seg", exist_ok=True)\r\n        os.makedirs(f"{output_dir}/annotations", exist_ok=True)\r\n\r\n    def setup_sensors_for_data_collection(self, world: World):\r\n        """Setup sensors for comprehensive data collection"""\r\n\r\n        # Create RGB camera\r\n        self.rgb_camera = Camera(\r\n            prim_path="/World/RGB_Camera",\r\n            name="rgb_camera",\r\n            position=np.array([0.3, 0, 0.8]),\r\n            frequency=30\r\n        )\r\n        self.rgb_camera.add_render_product(resolution=(640, 480))\r\n\r\n        # Create depth camera\r\n        self.depth_camera = Camera(\r\n            prim_path="/World/Depth_Camera",\r\n            name="depth_camera",\r\n            position=np.array([0.3, 0, 0.8]),\r\n            frequency=30\r\n        )\r\n        self.depth_camera.add_render_product(resolution=(640, 480))\r\n\r\n        # Create semantic segmentation camera\r\n        self.seg_camera = Camera(\r\n            prim_path="/World/Seg_Camera",\r\n            name="seg_camera",\r\n            position=np.array([0.3, 0, 0.8]),\r\n            frequency=30\r\n        )\r\n        self.seg_camera.add_render_product(resolution=(640, 480))\r\n\r\n        # Add LiDAR sensor\r\n        self.lidar = LidarRtx(\r\n            prim_path="/World/Lidar",\r\n            name="Lidar",\r\n            translation=np.array([0.2, 0, 0.9]),\r\n            orientation=np.array([0, 0, 0, 1]),\r\n            config="Example_Rotary_Mechanical_Lidar",\r\n            depth_clamp_near=0.1,\r\n            depth_clamp_far=100,\r\n            horizontal_resolution=0.5,\r\n            vertical_resolution=0.25,\r\n            horizontal_lasers=1024,\r\n            vertical_lasers=64,\r\n            max_range=100,\r\n            min_range=0.1,\r\n        )\r\n\r\n        # Initialize sensors\r\n        world.scene.add(self.rgb_camera)\r\n        world.scene.add(self.depth_camera)\r\n        world.scene.add(self.seg_camera)\r\n        world.scene.add(self.lidar)\r\n\r\n    def capture_multimodal_data(self) -> Dict:\r\n        """Capture synchronized multimodal sensor data"""\r\n\r\n        # Capture RGB image\r\n        rgb_data = self.rgb_camera.get_render_product().get_texture()\r\n        rgb_image = Image.fromarray(rgb_data, mode="RGB")\r\n\r\n        # Capture depth data\r\n        depth_data = self.depth_camera.get_current_depth()\r\n\r\n        # Capture segmentation data\r\n        seg_data = self.seg_camera.get_current_segmentation()\r\n\r\n        # Capture LiDAR data\r\n        lidar_data = self.lidar.get_linear_depth_data()\r\n\r\n        # Get robot state for annotations\r\n        robot_state = self.get_robot_state()\r\n\r\n        # Create annotation\r\n        annotation = {\r\n            \'timestamp\': self.data_counter,\r\n            \'image_path\': f"images/frame_{self.data_counter:06d}.png",\r\n            \'depth_path\': f"depth/depth_{self.data_counter:06d}.npy",\r\n            \'seg_path\': f"seg/seg_{self.data_counter:06d}.png",\r\n            \'lidar_path\': f"lidar/lidar_{self.data_counter:06d}.npy",\r\n            \'robot_state\': robot_state,\r\n            \'objects\': self.get_scene_objects(),\r\n            \'camera_pose\': self.get_camera_pose()\r\n        }\r\n\r\n        self.annotation_data.append(annotation)\r\n\r\n        return {\r\n            \'rgb\': rgb_image,\r\n            \'depth\': depth_data,\r\n            \'segmentation\': seg_data,\r\n            \'lidar\': lidar_data,\r\n            \'annotation\': annotation\r\n        }\r\n\r\n    def save_multimodal_sample(self, data: Dict):\r\n        """Save a complete multimodal data sample"""\r\n\r\n        # Save RGB image\r\n        rgb_path = f"{self.output_dir}/{data[\'annotation\'][\'image_path\']}"\r\n        data[\'rgb\'].save(rgb_path)\r\n\r\n        # Save depth data\r\n        depth_path = f"{self.output_dir}/{data[\'annotation\'][\'depth_path\']}"\r\n        np.save(depth_path, data[\'depth\'])\r\n\r\n        # Save segmentation\r\n        seg_path = f"{self.output_dir}/{data[\'annotation\'][\'seg_path\']}"\r\n        seg_image = Image.fromarray(data[\'segmentation\'].astype(np.uint8))\r\n        seg_image.save(seg_path)\r\n\r\n        # Save LiDAR data\r\n        lidar_path = f"{self.output_dir}/{data[\'annotation\'][\'lidar_path\']}"\r\n        np.save(lidar_path, data[\'lidar\'])\r\n\r\n        # Update counter\r\n        self.data_counter += 1\r\n\r\n    def get_robot_state(self):\r\n        """Get current robot state for annotation"""\r\n        # This would interface with the robot in the simulation\r\n        # For now, return example state\r\n        return {\r\n            \'position\': [0.0, 0.0, 0.0],\r\n            \'orientation\': [0.0, 0.0, 0.0, 1.0],  # quaternion\r\n            \'joint_positions\': [0.0] * 12,  # Example for 12 DOF\r\n            \'joint_velocities\': [0.0] * 12\r\n        }\r\n\r\n    def get_scene_objects(self):\r\n        """Get information about objects in the scene"""\r\n        # This would scan the USD stage for objects\r\n        # For now, return example objects\r\n        return [\r\n            {\r\n                \'name\': \'table\',\r\n                \'class\': \'furniture\',\r\n                \'bbox\': {\'min\': [-1, -0.5, 0], \'max\': [1, 0.5, 0.8]},\r\n                \'pose\': {\'position\': [0, 0, 0.4], \'orientation\': [0, 0, 0, 1]}\r\n            },\r\n            {\r\n                \'name\': \'cube\',\r\n                \'class\': \'object\',\r\n                \'bbox\': {\'min\': [0.5, 0.2, 0.1], \'max\': [0.7, 0.4, 0.3]},\r\n                \'pose\': {\'position\': [0.6, 0.3, 0.2], \'orientation\': [0, 0, 0, 1]}\r\n            }\r\n        ]\r\n\r\n    def get_camera_pose(self):\r\n        """Get camera pose for annotation"""\r\n        # Get camera transform from USD\r\n        return {\r\n            \'position\': [0.3, 0, 0.8],\r\n            \'orientation\': [0.707, 0, 0, 0.707]  # Looking forward\r\n        }\r\n\r\n    def generate_dataset(self, num_samples: int = 1000,\r\n                        domain_randomization: bool = True):\r\n        """Generate synthetic dataset with domain randomization"""\r\n\r\n        print(f"Generating {num_samples} synthetic samples...")\r\n\r\n        for i in range(num_samples):\r\n            if domain_randomization and i % 50 == 0:  # Randomize every 50 samples\r\n                self.apply_domain_randomization()\r\n\r\n            # Capture data\r\n            data = self.capture_multimodal_data()\r\n\r\n            # Save sample\r\n            self.save_multimodal_sample(data)\r\n\r\n            # Progress indicator\r\n            if i % 100 == 0:\r\n                print(f"Generated {i}/{num_samples} samples")\r\n\r\n        # Save annotations\r\n        self.save_annotations()\r\n\r\n        print(f"Dataset generation complete! Generated {self.data_counter} samples")\r\n\r\n    def apply_domain_randomization(self):\r\n        """Apply domain randomization to the scene"""\r\n        # This would randomize lighting, materials, textures, etc.\r\n        # In practice, this would use Isaac Sim\'s domain randomization tools\r\n        print("Applying domain randomization...")\r\n\r\n    def save_annotations(self):\r\n        """Save annotation data to JSON file"""\r\n        annotations_path = f"{self.output_dir}/annotations/dataset_annotations.json"\r\n        with open(annotations_path, \'w\') as f:\r\n            json.dump(self.annotation_data, f, indent=2)\r\n\r\nclass DomainRandomizationManager:\r\n    """Manage domain randomization for synthetic data generation"""\r\n\r\n    def __init__(self, world: World):\r\n        self.world = world\r\n        self.randomization_config = self.get_default_config()\r\n\r\n    def get_default_config(self) -> Dict:\r\n        """Get default domain randomization configuration"""\r\n        return {\r\n            \'lighting\': {\r\n                \'intensity_range\': (1000, 8000),\r\n                \'color_temperature_range\': (3000, 8000),  # Kelvin\r\n                \'position_range\': ([-10, -10, 5], [10, 10, 15])\r\n            },\r\n            \'materials\': {\r\n                \'roughness_range\': (0.0, 1.0),\r\n                \'metallic_range\': (0.0, 1.0),\r\n                \'albedo_range\': ([0.1, 0.1, 0.1], [1.0, 1.0, 1.0])\r\n            },\r\n            \'textures\': {\r\n                \'randomize_albedo\': True,\r\n                \'randomize_normal\': True,\r\n                \'randomize_roughness\': True,\r\n                \'texture_library\': [\r\n                    \'wood\', \'metal\', \'plastic\', \'fabric\', \'stone\', \'grass\'\r\n                ]\r\n            },\r\n            \'environment\': {\r\n                \'randomize_backgrounds\': True,\r\n                \'randomize_objects\': True,\r\n                \'object_count_range\': (5, 20),\r\n                \'object_size_range\': (0.05, 0.5)\r\n            }\r\n        }\r\n\r\n    def randomize_scene(self):\r\n        """Apply domain randomization to the current scene"""\r\n\r\n        # Randomize lighting\r\n        self.randomize_lighting()\r\n\r\n        # Randomize materials\r\n        self.randomize_materials()\r\n\r\n        # Randomize textures\r\n        self.randomize_textures()\r\n\r\n        # Randomize environment\r\n        self.randomize_environment()\r\n\r\n    def randomize_lighting(self):\r\n        """Randomize lighting conditions"""\r\n        # Find all lights in the scene\r\n        stage = omni.usd.get_context().get_stage()\r\n\r\n        for prim in stage.Traverse():\r\n            if prim.GetTypeName() in ["DistantLight", "SphereLight", "DiskLight", "DomeLight"]:\r\n                # Randomize intensity\r\n                intensity_range = self.randomization_config[\'lighting\'][\'intensity_range\']\r\n                new_intensity = np.random.uniform(*intensity_range)\r\n                prim.GetAttribute("inputs:intensity").Set(new_intensity)\r\n\r\n                # Randomize color (based on temperature)\r\n                temp_range = self.randomization_config[\'lighting\'][\'color_temperature_range\']\r\n                temp = np.random.uniform(*temp_range)\r\n                color = self.temperature_to_rgb(temp)\r\n                prim.GetAttribute("inputs:color").Set(color)\r\n\r\n    def temperature_to_rgb(self, kelvin: float) -> Tuple[float, float, float]:\r\n        """Convert temperature in Kelvin to RGB color"""\r\n        # Simplified approximation\r\n        temp = kelvin / 100\r\n\r\n        if temp <= 66:\r\n            red = 255\r\n            green = temp\r\n            green = 99.4708025861 * np.log(green) - 161.1195681661\r\n        else:\r\n            red = temp - 60\r\n            red = 329.698727446 * (red ** -0.1332047592)\r\n            green = temp - 60\r\n            green = 288.1221695283 * (green ** -0.0755148492)\r\n\r\n        blue = temp - 10\r\n        blue = 138.5177312231 * np.log(blue) - 305.0447927307\r\n\r\n        # Clamp values to [0, 255] then convert to [0, 1]\r\n        red = np.clip(red, 0, 255) / 255.0\r\n        green = np.clip(green, 0, 255) / 255.0\r\n        blue = np.clip(blue, 0, 255) / 255.0\r\n\r\n        return (red, green, blue)\r\n\r\n    def randomize_materials(self):\r\n        """Randomize material properties"""\r\n        # This would iterate through materials and randomize their properties\r\n        # For now, this is a placeholder\r\n        pass\r\n\r\n    def randomize_textures(self):\r\n        """Randomize textures on objects"""\r\n        # This would apply random textures to objects\r\n        # For now, this is a placeholder\r\n        pass\r\n\r\n    def randomize_environment(self):\r\n        """Randomize environment objects"""\r\n        # This would add/remove/randomize objects in the environment\r\n        # For now, this is a placeholder\r\n        pass\r\n\r\ndef setup_synthetic_data_pipeline():\r\n    """Setup the complete synthetic data generation pipeline"""\r\n\r\n    # Initialize world\r\n    world = World(stage_units_in_meters=1.0)\r\n\r\n    # Add ground plane\r\n    world.scene.add_default_ground_plane()\r\n\r\n    # Setup domain randomization\r\n    dr_manager = DomainRandomizationManager(world)\r\n\r\n    # Setup synthetic data manager\r\n    data_manager = SyntheticDataManager("synthetic_robotics_dataset")\r\n    data_manager.setup_sensors_for_data_collection(world)\r\n\r\n    # Add a simple robot for data collection\r\n    add_reference_to_stage(\r\n        usd_path=f"{get_assets_root_path()}/Isaac/Robots/Franka/franka_alt_fingers.usd",\r\n        prim_path="/World/Robot"\r\n    )\r\n\r\n    return world, data_manager, dr_manager\r\n\r\ndef main():\r\n    # Setup the synthetic data pipeline\r\n    world, data_manager, dr_manager = setup_synthetic_data_pipeline()\r\n\r\n    # Play the simulation\r\n    world.play()\r\n\r\n    # Generate synthetic dataset\r\n    data_manager.generate_dataset(num_samples=500, domain_randomization=True)\r\n\r\n    # Stop simulation\r\n    world.stop()\r\n\r\n    print("Synthetic data generation complete!")\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"sensor-simulation-and-calibration",children:"Sensor Simulation and Calibration"}),"\n",(0,t.jsx)(n.h3,{id:"realistic-sensor-simulation",children:"Realistic Sensor Simulation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nfrom scipy.spatial.transform import Rotation as R\r\nimport cv2\r\nfrom typing import Dict, Tuple, Optional\r\n\r\nclass RealisticCameraSimulator:\r\n    """Simulate realistic camera behavior with noise and distortions"""\r\n\r\n    def __init__(self,\r\n                 width: int = 640,\r\n                 height: int = 480,\r\n                 fov: float = 60.0,  # degrees\r\n                 noise_std: float = 0.01,\r\n                 pixel_size: float = 3.45e-6):  # 3.45 \u03bcm for typical camera\r\n        self.width = width\r\n        self.height = height\r\n        self.fov = fov\r\n        self.noise_std = noise_std\r\n        self.pixel_size = pixel_size  # meters per pixel\r\n\r\n        # Calculate camera intrinsic matrix\r\n        f = (self.width / 2) / np.tan(np.radians(self.fov / 2))\r\n        self.fx = f\r\n        self.fy = f\r\n        self.cx = self.width / 2\r\n        self.cy = self.height / 2\r\n\r\n        self.K = np.array([\r\n            [self.fx, 0, self.cx],\r\n            [0, self.fy, self.cy],\r\n            [0, 0, 1]\r\n        ])\r\n\r\n        # Distortion coefficients (k1, k2, p1, p2, k3)\r\n        self.distortion_coeffs = np.array([0.1, -0.2, 0.005, -0.005, 0.1])\r\n\r\n    def add_shot_noise(self, image: np.ndarray) -> np.ndarray:\r\n        """Add shot noise (photon noise) to image"""\r\n        # Shot noise is Poisson distributed, approximated as Gaussian for high photon counts\r\n        # Variance is proportional to signal intensity\r\n        signal = np.clip(image.astype(np.float32) / 255.0, 0, 1)\r\n        variance = signal * self.noise_std**2\r\n        noise = np.random.normal(0, np.sqrt(variance))\r\n        noisy_image = np.clip(signal + noise, 0, 1) * 255\r\n        return noisy_image.astype(np.uint8)\r\n\r\n    def add_read_noise(self, image: np.ndarray) -> np.ndarray:\r\n        """Add read noise to image"""\r\n        read_noise = np.random.normal(0, self.noise_std * 10, image.shape)\r\n        noisy_image = np.clip(image.astype(np.float32) + read_noise, 0, 255)\r\n        return noisy_image.astype(np.uint8)\r\n\r\n    def apply_lens_distortion(self, image: np.ndarray) -> np.ndarray:\r\n        """Apply lens distortion to image"""\r\n        h, w = image.shape[:2]\r\n\r\n        # Generate coordinate grids\r\n        x, y = np.meshgrid(np.arange(w), np.arange(h))\r\n        points = np.stack([x, y], axis=-1).reshape(-1, 2).astype(np.float32)\r\n\r\n        # Normalize points to camera coordinates\r\n        points_norm = (points - np.array([self.cx, self.cy])) / np.array([self.fx, self.fy])\r\n\r\n        # Apply distortion\r\n        k1, k2, p1, p2, k3 = self.distortion_coeffs\r\n\r\n        r2 = points_norm[:, 0]**2 + points_norm[:, 1]**2\r\n        r4 = r2**2\r\n        r6 = r2**3\r\n\r\n        radial_factor = 1 + k1*r2 + k2*r4 + k3*r6\r\n        tangential_x = 2*p1*points_norm[:, 0]*points_norm[:, 1] + p2*(r2 + 2*points_norm[:, 0]**2)\r\n        tangential_y = p1*(r2 + 2*points_norm[:, 1]**2) + 2*p2*points_norm[:, 0]*points_norm[:, 1]\r\n\r\n        distorted_points_norm = points_norm.copy()\r\n        distorted_points_norm[:, 0] = points_norm[:, 0] * radial_factor + tangential_x\r\n        distorted_points_norm[:, 1] = points_norm[:, 1] * radial_factor + tangential_y\r\n\r\n        # Convert back to pixel coordinates\r\n        distorted_points = distorted_points_norm * np.array([self.fx, self.fy]) + np.array([self.cx, self.cy])\r\n\r\n        # Reshape for OpenCV\r\n        distorted_points = distorted_points.reshape(h, w, 2)\r\n\r\n        # Remap image\r\n        map_x = distorted_points[:, :, 0].astype(np.float32)\r\n        map_y = distorted_points[:, :, 1].astype(np.float32)\r\n\r\n        undistorted_image = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR,\r\n                                     borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\r\n\r\n        return undistorted_image\r\n\r\n    def simulate_motion_blur(self, image: np.ndarray, motion_vector: Tuple[float, float]) -> np.ndarray:\r\n        """Simulate motion blur based on motion between frames"""\r\n        dx, dy = motion_vector\r\n\r\n        # Calculate kernel size based on motion magnitude\r\n        kernel_size = max(1, int(np.sqrt(dx**2 + dy**2) * 10))\r\n        if kernel_size <= 1:\r\n            return image\r\n\r\n        # Create motion blur kernel\r\n        kernel = np.zeros((kernel_size, kernel_size))\r\n        if dx != 0 or dy != 0:\r\n            # Normalize motion vector\r\n            length = np.sqrt(dx**2 + dy**2)\r\n            nx, ny = dx/length, dy/length\r\n\r\n            # Create line kernel in the direction of motion\r\n            center = kernel_size // 2\r\n            for i in range(kernel_size):\r\n                x = int(center + nx * (i - center))\r\n                y = int(center + ny * (i - center))\r\n                if 0 <= x < kernel_size and 0 <= y < kernel_size:\r\n                    kernel[y, x] = 1\r\n\r\n            kernel = kernel / np.sum(kernel)\r\n\r\n        # Apply convolution\r\n        if len(image.shape) == 3:\r\n            # Apply to each channel separately\r\n            blurred = np.zeros_like(image)\r\n            for c in range(image.shape[2]):\r\n                blurred[:, :, c] = cv2.filter2D(image[:, :, c], -1, kernel)\r\n        else:\r\n            blurred = cv2.filter2D(image, -1, kernel)\r\n\r\n        return blurred\r\n\r\n    def add_defocus_blur(self, image: np.ndarray, blur_radius: float) -> np.ndarray:\r\n        """Add defocus blur for out-of-focus regions"""\r\n        if blur_radius <= 0:\r\n            return image\r\n\r\n        kernel_size = max(3, int(blur_radius * 10))\r\n        if kernel_size % 2 == 0:\r\n            kernel_size += 1  # Make odd\r\n\r\n        # Create circular kernel for defocus blur\r\n        kernel = np.zeros((kernel_size, kernel_size))\r\n        center = kernel_size // 2\r\n        for i in range(kernel_size):\r\n            for j in range(kernel_size):\r\n                dist = np.sqrt((i - center)**2 + (j - center)**2)\r\n                if dist <= kernel_size // 2:\r\n                    kernel[i, j] = 1\r\n\r\n        kernel = kernel / np.sum(kernel)\r\n\r\n        if len(image.shape) == 3:\r\n            # Apply to each channel separately\r\n            blurred = np.zeros_like(image)\r\n            for c in range(image.shape[2]):\r\n                blurred[:, :, c] = cv2.filter2D(image[:, :, c], -1, kernel)\r\n        else:\r\n            blurred = cv2.filter2D(image, -1, kernel)\r\n\r\n        return blurred\r\n\r\n    def simulate_camera_image(self,\r\n                             scene_image: np.ndarray,\r\n                             motion_vector: Tuple[float, float] = (0, 0),\r\n                             focus_distance: float = 1.0) -> np.ndarray:\r\n        """Simulate complete camera pipeline with all effects"""\r\n        # Start with original image\r\n        simulated_image = scene_image.copy()\r\n\r\n        # Apply lens distortion\r\n        simulated_image = self.apply_lens_distortion(simulated_image)\r\n\r\n        # Add shot noise\r\n        simulated_image = self.add_shot_noise(simulated_image)\r\n\r\n        # Add read noise\r\n        simulated_image = self.add_read_noise(simulated_image)\r\n\r\n        # Apply motion blur\r\n        simulated_image = self.simulate_motion_blur(simulated_image, motion_vector)\r\n\r\n        # Add defocus blur based on focus distance\r\n        # For simplicity, assume blur increases with distance from focus\r\n        blur_radius = max(0, abs(1.0 - focus_distance) * 0.5)\r\n        simulated_image = self.add_defocus_blur(simulated_image, blur_radius)\r\n\r\n        return simulated_image\r\n\r\nclass RealisticLidarSimulator:\r\n    """Simulate realistic LiDAR sensor with noise and dropouts"""\r\n\r\n    def __init__(self,\r\n                 max_range: float = 25.0,\r\n                 min_range: float = 0.1,\r\n                 angular_resolution: float = 0.25,  # degrees\r\n                 noise_std: float = 0.01,\r\n                 dropout_probability: float = 0.01):\r\n        self.max_range = max_range\r\n        self.min_range = min_range\r\n        self.angular_resolution = np.radians(angular_resolution)\r\n        self.noise_std = noise_std\r\n        self.dropout_probability = dropout_probability\r\n\r\n    def add_measurement_noise(self, ranges: np.ndarray) -> np.ndarray:\r\n        """Add realistic measurement noise to LiDAR ranges"""\r\n        # Add Gaussian noise proportional to distance (more noise at longer ranges)\r\n        noise_factor = 1.0 + ranges / self.max_range  # More noise at longer distances\r\n        noise = np.random.normal(0, self.noise_std * noise_factor, ranges.shape)\r\n        noisy_ranges = ranges + noise\r\n\r\n        # Ensure valid range values\r\n        noisy_ranges = np.clip(noisy_ranges, self.min_range, self.max_range)\r\n\r\n        return noisy_ranges\r\n\r\n    def simulate_beam_divergence(self, ranges: np.ndarray, beam_width: float = 0.002) -> np.ndarray:\r\n        """Simulate beam divergence effect (returns from multiple surfaces in beam)"""\r\n        # In reality, this would require more complex modeling of beam shape\r\n        # For now, we\'ll add a small amount of variation based on beam width\r\n        beam_effect = np.random.normal(0, beam_width, ranges.shape)\r\n        return ranges + beam_effect\r\n\r\n    def simulate_dropouts(self, ranges: np.ndarray) -> np.ndarray:\r\n        """Simulate LiDAR dropouts (missing returns)"""\r\n        # Randomly set some ranges to max range (dropout)\r\n        dropouts = np.random.random(ranges.shape) < self.dropout_probability\r\n        dropout_ranges = ranges.copy()\r\n        dropout_ranges[dropouts] = self.max_range\r\n\r\n        return dropout_ranges\r\n\r\n    def simulate_multiple_returns(self, ranges: np.ndarray) -> np.ndarray:\r\n        """Simulate multiple returns for partially reflective surfaces"""\r\n        # This is a simplified simulation - in reality, LiDAR can return multiple hits per beam\r\n        # For now, we\'ll just add some variation\r\n        multiple_returns = np.zeros_like(ranges)\r\n\r\n        for i in range(len(ranges)):\r\n            if ranges[i] < self.max_range and np.random.random() < 0.1:  # 10% chance of multiple returns\r\n                # Add a secondary return closer than the primary\r\n                secondary_return = ranges[i] * np.random.uniform(0.7, 0.95)\r\n                multiple_returns[i] = secondary_return\r\n            else:\r\n                multiple_returns[i] = ranges[i]\r\n\r\n        return multiple_returns\r\n\r\n    def simulate_lidar_scan(self,\r\n                           ground_truth_ranges: np.ndarray,\r\n                           surface_properties: Optional[np.ndarray] = None) -> np.ndarray:\r\n        """Simulate complete LiDAR scan with all realistic effects"""\r\n        simulated_scan = ground_truth_ranges.copy()\r\n\r\n        # Add measurement noise\r\n        simulated_scan = self.add_measurement_noise(simulated_scan)\r\n\r\n        # Simulate beam divergence\r\n        simulated_scan = self.simulate_beam_divergence(simulated_scan)\r\n\r\n        # Simulate dropouts\r\n        simulated_scan = self.simulate_dropouts(simulated_scan)\r\n\r\n        # Simulate multiple returns\r\n        simulated_scan = self.simulate_multiple_returns(simulated_scan)\r\n\r\n        return simulated_scan\r\n\r\nclass IMUSimulator:\r\n    """Simulate realistic IMU sensor data with bias and noise"""\r\n\r\n    def __init__(self,\r\n                 accel_noise_density: float = 0.002,  # m/s^2 / sqrt(Hz)\r\n                 gyro_noise_density: float = 0.0001,   # rad/s / sqrt(Hz)\r\n                 accel_bias_instability: float = 0.005,  # m/s^2\r\n                 gyro_bias_instability: float = 0.00001,  # rad/s\r\n                 sampling_rate: float = 100.0):  # Hz\r\n        self.accel_noise_density = accel_noise_density\r\n        self.gyro_noise_density = gyro_noise_density\r\n        self.accel_bias_instability = accel_bias_instability\r\n        self.gyro_bias_instability = gyro_bias_instability\r\n        self.sampling_rate = sampling_rate\r\n\r\n        # Initialize biases (random walk process)\r\n        self.accel_bias = np.random.normal(0, accel_bias_instability, 3)\r\n        self.gyro_bias = np.random.normal(0, gyro_bias_instability, 3)\r\n\r\n        # Allan variance constants for bias random walk\r\n        self.accel_bias_rw = accel_bias_instability / np.sqrt(sampling_rate)\r\n        self.gyro_bias_rw = gyro_bias_instability / np.sqrt(sampling_rate)\r\n\r\n    def update_biases(self, dt: float):\r\n        """Update biases using random walk model"""\r\n        # Accelerometer bias random walk\r\n        self.accel_bias += np.random.normal(0, self.accel_bias_rw * np.sqrt(dt), 3)\r\n\r\n        # Gyroscope bias random walk\r\n        self.gyro_bias += np.random.normal(0, self.gyro_bias_rw * np.sqrt(dt), 3)\r\n\r\n    def simulate_accelerometer(self, true_accel: np.ndarray, dt: float) -> np.ndarray:\r\n        """Simulate accelerometer measurement"""\r\n        # Update biases\r\n        self.update_biases(dt)\r\n\r\n        # Add noise (white noise + bias)\r\n        accel_noise = np.random.normal(0, self.accel_noise_density / np.sqrt(dt), 3)\r\n\r\n        # Simulate accelerometer measurement\r\n        measured_accel = true_accel + self.accel_bias + accel_noise\r\n\r\n        return measured_accel\r\n\r\n    def simulate_gyroscope(self, true_angular_velocity: np.ndarray, dt: float) -> np.ndarray:\r\n        """Simulate gyroscope measurement"""\r\n        # Update biases\r\n        self.update_biases(dt)\r\n\r\n        # Add noise (white noise + bias)\r\n        gyro_noise = np.random.normal(0, self.gyro_noise_density / np.sqrt(dt), 3)\r\n\r\n        # Simulate gyroscope measurement\r\n        measured_gyro = true_angular_velocity + self.gyro_bias + gyro_noise\r\n\r\n        return measured_gyro\r\n\r\n    def simulate_imu_reading(self,\r\n                           true_state: Dict[str, np.ndarray],\r\n                           dt: float) -> Dict[str, np.ndarray]:\r\n        """Simulate complete IMU reading"""\r\n        # Calculate true values\r\n        true_accel = true_state[\'linear_acceleration\']\r\n        true_gyro = true_state[\'angular_velocity\']\r\n\r\n        # Simulate measurements\r\n        accel_measurement = self.simulate_accelerometer(true_accel, dt)\r\n        gyro_measurement = self.simulate_gyroscope(true_gyro, dt)\r\n\r\n        return {\r\n            \'accelerometer\': accel_measurement,\r\n            \'gyroscope\': gyro_measurement,\r\n            \'timestamp\': true_state.get(\'timestamp\', 0.0)\r\n        }\n'})}),"\n",(0,t.jsx)(n.h2,{id:"transfer-learning-techniques",children:"Transfer Learning Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"domain-adaptation-methods",children:"Domain Adaptation Methods"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nimport numpy as np\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.decomposition import PCA\r\n\r\nclass DomainAdaptationNetwork(nn.Module):\r\n    """Neural network with domain adaptation capabilities"""\r\n\r\n    def __init__(self, input_dim, hidden_dims, output_dim, domain_adaptation=True):\r\n        super().__init__()\r\n\r\n        # Feature extractor (shared between domains)\r\n        layers = []\r\n        prev_dim = input_dim\r\n        for hidden_dim in hidden_dims:\r\n            layers.extend([\r\n                nn.Linear(prev_dim, hidden_dim),\r\n                nn.BatchNorm1d(hidden_dim),\r\n                nn.ReLU(),\r\n                nn.Dropout(0.5)\r\n            ])\r\n            prev_dim = hidden_dim\r\n\r\n        self.feature_extractor = nn.Sequential(*layers)\r\n\r\n        # Label classifier (task-specific)\r\n        self.label_classifier = nn.Sequential(\r\n            nn.Linear(prev_dim, prev_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.5),\r\n            nn.Linear(prev_dim // 2, output_dim)\r\n        )\r\n\r\n        # Domain classifier for adversarial training\r\n        self.domain_adaptation = domain_adaptation\r\n        if domain_adaptation:\r\n            self.domain_classifier = nn.Sequential(\r\n                GradientReversalLayer(),\r\n                nn.Linear(prev_dim, prev_dim // 4),\r\n                nn.ReLU(),\r\n                nn.Linear(prev_dim // 4, 1),\r\n                nn.Sigmoid()\r\n            )\r\n\r\n    def forward(self, x, alpha=1.0):\r\n        features = self.feature_extractor(x)\r\n\r\n        label_output = self.label_classifier(features)\r\n\r\n        domain_output = None\r\n        if self.domain_adaptation:\r\n            # Apply gradient reversal with alpha parameter\r\n            reversed_features = self.gradient_reversal(features, alpha)\r\n            domain_output = self.domain_classifier(reversed_features)\r\n\r\n        return label_output, domain_output\r\n\r\n    def gradient_reversal(self, x, alpha):\r\n        """Gradient reversal layer"""\r\n        return GradientReversalFunction.apply(x, alpha)\r\n\r\nclass GradientReversalFunction(torch.autograd.Function):\r\n    """Gradient reversal function"""\r\n\r\n    @staticmethod\r\n    def forward(ctx, input, alpha):\r\n        ctx.alpha = alpha\r\n        return input\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        output = grad_output.neg() * ctx.alpha\r\n        return output, None\r\n\r\nclass GradientReversalLayer(nn.Module):\r\n    """Gradient reversal layer module"""\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        return GradientReversalFunction.apply(x, 1.0)\r\n\r\nclass CycleConsistencyNetwork(nn.Module):\r\n    """Network implementing cycle consistency for domain adaptation"""\r\n\r\n    def __init__(self, input_dim, hidden_dim, latent_dim):\r\n        super().__init__()\r\n\r\n        # Encoder networks\r\n        self.synth_encoder = self._make_encoder(input_dim, hidden_dim, latent_dim)\r\n        self.real_encoder = self._make_encoder(input_dim, hidden_dim, latent_dim)\r\n\r\n        # Decoder networks\r\n        self.synth_decoder = self._make_decoder(latent_dim, hidden_dim, input_dim)\r\n        self.real_decoder = self._make_decoder(latent_dim, hidden_dim, input_dim)\r\n\r\n        # Discriminator networks\r\n        self.synth_discriminator = self._make_discriminator(input_dim, hidden_dim)\r\n        self.real_discriminator = self._make_discriminator(input_dim, hidden_dim)\r\n\r\n    def _make_encoder(self, input_dim, hidden_dim, latent_dim):\r\n        return nn.Sequential(\r\n            nn.Linear(input_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim // 2, latent_dim)\r\n        )\r\n\r\n    def _make_decoder(self, latent_dim, hidden_dim, output_dim):\r\n        return nn.Sequential(\r\n            nn.Linear(latent_dim, hidden_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim // 2, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, output_dim)\r\n        )\r\n\r\n    def _make_discriminator(self, input_dim, hidden_dim):\r\n        return nn.Sequential(\r\n            nn.Linear(input_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim // 2, 1),\r\n            nn.Sigmoid()\r\n        )\r\n\r\n    def forward(self, synth_data, real_data):\r\n        # Encode to latent space\r\n        synth_latent = self.synth_encoder(synth_data)\r\n        real_latent = self.real_encoder(real_data)\r\n\r\n        # Decode back to original space\r\n        synth_reconstructed = self.synth_decoder(synth_latent)\r\n        real_reconstructed = self.real_decoder(real_latent)\r\n\r\n        # Cross-domain reconstruction\r\n        synth_to_real = self.real_decoder(synth_latent)\r\n        real_to_synth = self.synth_decoder(real_latent)\r\n\r\n        # Reconstruct back to original domains\r\n        cycle_synth = self.synth_encoder(real_to_synth)\r\n        cycle_real = self.real_encoder(synth_to_real)\r\n\r\n        # Discriminator outputs\r\n        synth_discrim = self.synth_discriminator(synth_data)\r\n        real_discrim = self.real_discriminator(real_data)\r\n\r\n        return {\r\n            \'synth_reconstructed\': synth_reconstructed,\r\n            \'real_reconstructed\': real_reconstructed,\r\n            \'synth_to_real\': synth_to_real,\r\n            \'real_to_synth\': real_to_synth,\r\n            \'cycle_synth\': cycle_synth,\r\n            \'cycle_real\': cycle_real,\r\n            \'synth_discrim\': synth_discrim,\r\n            \'real_discrim\': real_discrim\r\n        }\r\n\r\nclass FeatureAlignmentLoss(nn.Module):\r\n    """Loss function for aligning feature distributions between domains"""\r\n\r\n    def __init__(self, method=\'mmd\', kernel_type=\'rbf\'):\r\n        super().__init__()\r\n        self.method = method\r\n        self.kernel_type = kernel_type\r\n\r\n    def forward(self, source_features, target_features):\r\n        if self.method == \'mmd\':\r\n            return self.compute_mmd_loss(source_features, target_features)\r\n        elif self.method == \'correlation\':\r\n            return self.compute_correlation_loss(source_features, target_features)\r\n        else:\r\n            raise ValueError(f"Unknown method: {self.method}")\r\n\r\n    def compute_mmd_loss(self, source, target):\r\n        """Compute Maximum Mean Discrepancy loss"""\r\n        if self.kernel_type == \'rbf\':\r\n            return self._rbf_mmd_loss(source, target)\r\n        else:\r\n            raise ValueError(f"Unknown kernel type: {self.kernel_type}")\r\n\r\n    def _rbf_mmd_loss(self, source, target):\r\n        """Compute RBF MMD loss"""\r\n        XX = torch.matmul(source, source.t())\r\n        YY = torch.matmul(target, target.t())\r\n        XY = torch.matmul(source, target.t())\r\n\r\n        source_norm = torch.diag(XX)\r\n        target_norm = torch.diag(YY)\r\n\r\n        source_norm_expand = source_norm.unsqueeze(1).expand(-1, source.size(0))\r\n        target_norm_expand = target_norm.unsqueeze(1).expand(-1, target.size(0))\r\n\r\n        XX_expand = source_norm_expand + source_norm_expand.t() - 2 * XX\r\n        YY_expand = target_norm_expand + target_norm_expand.t() - 2 * YY\r\n        XY_expand = source_norm_expand + target_norm_expand.t() - 2 * XY\r\n\r\n        gamma = 1.0 / (2 * source.size(1) * torch.mean(XX_expand))\r\n\r\n        K_XX = torch.exp(-gamma * XX_expand)\r\n        K_YY = torch.exp(-gamma * YY_expand)\r\n        K_XY = torch.exp(-gamma * XY_expand)\r\n\r\n        mmd_loss = torch.mean(K_XX) + torch.mean(K_YY) - 2 * torch.mean(K_XY)\r\n\r\n        return mmd_loss\r\n\r\n    def compute_correlation_loss(self, source, target):\r\n        """Compute correlation alignment loss"""\r\n        source_mean = torch.mean(source, dim=0, keepdim=True)\r\n        target_mean = torch.mean(target, dim=0, keepdim=True)\r\n\r\n        source_centered = source - source_mean\r\n        target_centered = target - target_mean\r\n\r\n        source_cov = torch.matmul(source_centered.t(), source_centered) / (source.size(0) - 1)\r\n        target_cov = torch.matmul(target_centered.t(), target_centered) / (target.size(0) - 1)\r\n\r\n        # Compute Frobenius norm of difference\r\n        correlation_loss = torch.norm(source_cov - target_cov, p=\'fro\')\r\n\r\n        return correlation_loss\r\n\r\nclass Sim2RealTrainer:\r\n    """Training framework for sim-to-real transfer"""\r\n\r\n    def __init__(self, model, device=\'cuda\'):\r\n        self.model = model.to(device)\r\n        self.device = device\r\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\r\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)\r\n\r\n        # Loss functions\r\n        self.task_loss_fn = nn.CrossEntropyLoss()\r\n        self.domain_loss_fn = nn.BCELoss()\r\n        self.feature_alignment_loss = FeatureAlignmentLoss(method=\'mmd\')\r\n\r\n        # Training metrics\r\n        self.training_history = {\r\n            \'task_loss\': [],\r\n            \'domain_loss\': [],\r\n            \'alignment_loss\': [],\r\n            \'total_loss\': []\r\n        }\r\n\r\n    def train_epoch(self, synth_loader, real_loader, epoch):\r\n        """Train for one epoch with domain adaptation"""\r\n        self.model.train()\r\n\r\n        task_losses = []\r\n        domain_losses = []\r\n        alignment_losses = []\r\n        total_losses = []\r\n\r\n        # Get iterators for both domains\r\n        synth_iter = iter(synth_loader)\r\n        real_iter = iter(real_loader)\r\n\r\n        # Determine the length of the shorter loader\r\n        num_batches = min(len(synth_loader), len(real_loader))\r\n\r\n        for _ in range(num_batches):\r\n            try:\r\n                # Get batch from synthetic data\r\n                synth_batch = next(synth_iter)\r\n                synth_data, synth_labels = synth_batch[0].to(self.device), synth_batch[1].to(self.device)\r\n\r\n                # Get batch from real data\r\n                real_batch = next(real_iter)\r\n                real_data, real_labels = real_batch[0].to(self.device), real_batch[1].to(self.device)\r\n\r\n                # Combine batches for processing\r\n                combined_data = torch.cat([synth_data, real_data], dim=0)\r\n                domain_labels = torch.cat([\r\n                    torch.zeros(synth_data.size(0)),  # Synthetic domain = 0\r\n                    torch.ones(real_data.size(0))     # Real domain = 1\r\n                ]).to(self.device)\r\n\r\n                # Compute alpha for gradient reversal (increases during training)\r\n                alpha = 2.0 / (1.0 + np.exp(-10 * epoch / 100)) - 1.0\r\n\r\n                # Forward pass\r\n                task_preds, domain_preds = self.model(combined_data, alpha)\r\n\r\n                # Task prediction loss (supervised on synthetic, unsupervised on real)\r\n                synth_task_preds = task_preds[:synth_data.size(0)]\r\n                task_loss = self.task_loss_fn(synth_task_preds, synth_labels)\r\n\r\n                # Domain classification loss\r\n                domain_loss = self.domain_loss_fn(domain_preds.squeeze(), domain_labels)\r\n\r\n                # Feature alignment loss\r\n                synth_features = self.model.feature_extractor(synth_data)\r\n                real_features = self.model.feature_extractor(real_data)\r\n                alignment_loss = self.feature_alignment_loss(synth_features, real_features)\r\n\r\n                # Total loss\r\n                total_loss = task_loss + 0.5 * domain_loss + 0.3 * alignment_loss\r\n\r\n                # Backward pass\r\n                self.optimizer.zero_grad()\r\n                total_loss.backward()\r\n                self.optimizer.step()\r\n\r\n                # Store losses\r\n                task_losses.append(task_loss.item())\r\n                domain_losses.append(domain_loss.item())\r\n                alignment_losses.append(alignment_loss.item())\r\n                total_losses.append(total_loss.item())\r\n\r\n            except StopIteration:\r\n                break\r\n\r\n        # Update metrics\r\n        self.training_history[\'task_loss\'].append(np.mean(task_losses))\r\n        self.training_history[\'domain_loss\'].append(np.mean(domain_losses))\r\n        self.training_history[\'alignment_loss\'].append(np.mean(alignment_losses))\r\n        self.training_history[\'total_loss\'].append(np.mean(total_losses))\r\n\r\n        # Update scheduler\r\n        self.scheduler.step()\r\n\r\n        print(f"Epoch {epoch}: Task Loss: {np.mean(task_losses):.4f}, "\r\n              f"Domain Loss: {np.mean(domain_losses):.4f}, "\r\n              f"Alignment Loss: {np.mean(alignment_losses):.4f}")\r\n\r\n    def evaluate_transfer(self, real_test_loader):\r\n        """Evaluate model performance on real test data"""\r\n        self.model.eval()\r\n        correct = 0\r\n        total = 0\r\n\r\n        with torch.no_grad():\r\n            for data, labels in real_test_loader:\r\n                data, labels = data.to(self.device), labels.to(self.device)\r\n                task_preds, _ = self.model(data)\r\n                _, predicted = torch.max(task_preds.data, 1)\r\n                total += labels.size(0)\r\n                correct += (predicted == labels).sum().item()\r\n\r\n        accuracy = 100 * correct / total\r\n        print(f"Transfer accuracy on real test set: {accuracy:.2f}%")\r\n        return accuracy\r\n\r\ndef train_with_domain_adaptation(synth_dataset, real_dataset, model, epochs=50):\r\n    """Train model with domain adaptation for sim-to-real transfer"""\r\n\r\n    # Create data loaders\r\n    synth_loader = DataLoader(synth_dataset, batch_size=32, shuffle=True)\r\n    real_loader = DataLoader(real_dataset, batch_size=32, shuffle=True)\r\n    real_test_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\r\n\r\n    # Initialize trainer\r\n    trainer = Sim2RealTrainer(model)\r\n\r\n    # Train with domain adaptation\r\n    for epoch in range(epochs):\r\n        trainer.train_epoch(synth_loader, real_loader, epoch)\r\n\r\n    # Evaluate transfer performance\r\n    final_accuracy = trainer.evaluate_transfer(real_test_loader)\r\n\r\n    return final_accuracy\n'})}),"\n",(0,t.jsx)(n.h2,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,t.jsx)(n.h3,{id:"transfer-validation-framework",children:"Transfer Validation Framework"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nfrom scipy.spatial.distance import euclidean\r\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\nimport matplotlib.pyplot as plt\r\nfrom typing import Dict, List, Tuple\r\nimport seaborn as sns\r\n\r\nclass TransferValidator:\r\n    \"\"\"Validate sim-to-real transfer performance\"\"\"\r\n\r\n    def __init__(self):\r\n        self.sim_metrics = {}\r\n        self.real_metrics = {}\r\n        self.transfer_gap_analysis = {}\r\n\r\n    def validate_behavior_transfer(self,\r\n                                 sim_behavior_data: Dict,\r\n                                 real_behavior_data: Dict,\r\n                                 behavior_type: str = \"locomotion\") -> Dict:\r\n        \"\"\"Validate that behaviors transfer correctly from sim to real\"\"\"\r\n\r\n        if behavior_type == \"locomotion\":\r\n            return self._validate_locomotion_transfer(sim_behavior_data, real_behavior_data)\r\n        elif behavior_type == \"manipulation\":\r\n            return self._validate_manipulation_transfer(sim_behavior_data, real_behavior_data)\r\n        elif behavior_type == \"navigation\":\r\n            return self._validate_navigation_transfer(sim_behavior_data, real_behavior_data)\r\n        else:\r\n            raise ValueError(f\"Unknown behavior type: {behavior_type}\")\r\n\r\n    def _validate_locomotion_transfer(self, sim_data, real_data) -> Dict:\r\n        \"\"\"Validate locomotion behavior transfer\"\"\"\r\n        metrics = {}\r\n\r\n        # Compare gait patterns\r\n        sim_stride_lengths = sim_data.get('stride_lengths', [])\r\n        real_stride_lengths = real_data.get('stride_lengths', [])\r\n\r\n        if len(sim_stride_lengths) > 0 and len(real_stride_lengths) > 0:\r\n            mean_sim_stride = np.mean(sim_stride_lengths)\r\n            mean_real_stride = np.mean(real_stride_lengths)\r\n            stride_error = abs(mean_sim_stride - mean_real_stride) / mean_sim_stride\r\n            metrics['stride_length_error'] = stride_error\r\n\r\n        # Compare step frequencies\r\n        sim_step_freqs = sim_data.get('step_frequencies', [])\r\n        real_step_freqs = real_data.get('step_frequencies', [])\r\n\r\n        if len(sim_step_freqs) > 0 and len(real_step_freqs) > 0:\r\n            mean_sim_freq = np.mean(sim_step_freqs)\r\n            mean_real_freq = np.mean(real_step_freqs)\r\n            freq_error = abs(mean_sim_freq - mean_real_freq) / mean_sim_freq\r\n            metrics['step_frequency_error'] = freq_error\r\n\r\n        # Compare stability metrics\r\n        sim_com_variances = sim_data.get('com_variances', [])\r\n        real_com_variances = real_data.get('com_variances', [])\r\n\r\n        if len(sim_com_variances) > 0 and len(real_com_variances) > 0:\r\n            mean_sim_com_var = np.mean(sim_com_variances)\r\n            mean_real_com_var = np.mean(real_com_variances)\r\n            stability_error = abs(mean_sim_com_var - mean_real_com_var) / mean_sim_com_var\r\n            metrics['stability_error'] = stability_error\r\n\r\n        # Overall transfer score (weighted combination of metrics)\r\n        weights = {\r\n            'stride_length_error': 0.3,\r\n            'step_frequency_error': 0.4,\r\n            'stability_error': 0.3\r\n        }\r\n\r\n        overall_score = 0\r\n        for metric, weight in weights.items():\r\n            if metric in metrics:\r\n                # Convert error to score (lower error = higher score)\r\n                error = metrics[metric]\r\n                score = max(0, 1 - error)  # Score between 0 and 1\r\n                overall_score += score * weight\r\n\r\n        metrics['transfer_score'] = overall_score\r\n\r\n        return metrics\r\n\r\n    def _validate_manipulation_transfer(self, sim_data, real_data) -> Dict:\r\n        \"\"\"Validate manipulation behavior transfer\"\"\"\r\n        metrics = {}\r\n\r\n        # Compare reaching accuracy\r\n        sim_reach_errors = sim_data.get('reaching_errors', [])\r\n        real_reach_errors = real_data.get('reaching_errors', [])\r\n\r\n        if len(sim_reach_errors) > 0 and len(real_reach_errors) > 0:\r\n            mean_sim_error = np.mean(sim_reach_errors)\r\n            mean_real_error = np.mean(real_reach_errors)\r\n            reach_accuracy_error = abs(mean_sim_error - mean_real_error) / (mean_sim_error + 1e-6)\r\n            metrics['reaching_accuracy_error'] = reach_accuracy_error\r\n\r\n        # Compare grasp success rates\r\n        sim_grasp_success = sim_data.get('grasp_success_rates', [])\r\n        real_grasp_success = real_data.get('grasp_success_rates', [])\r\n\r\n        if len(sim_grasp_success) > 0 and len(real_grasp_success) > 0:\r\n            mean_sim_success = np.mean(sim_grasp_success)\r\n            mean_real_success = np.mean(real_grasp_success)\r\n            grasp_success_error = abs(mean_sim_success - mean_real_success) / (mean_sim_success + 1e-6)\r\n            metrics['grasp_success_error'] = grasp_success_error\r\n\r\n        # Compare execution time\r\n        sim_execution_times = sim_data.get('execution_times', [])\r\n        real_execution_times = real_data.get('execution_times', [])\r\n\r\n        if len(sim_execution_times) > 0 and len(real_execution_times) > 0:\r\n            mean_sim_time = np.mean(sim_execution_times)\r\n            mean_real_time = np.mean(real_execution_times)\r\n            time_error = abs(mean_real_time - mean_sim_time) / (mean_sim_time + 1e-6)\r\n            metrics['execution_time_error'] = time_error\r\n\r\n        # Overall transfer score\r\n        weights = {\r\n            'reaching_accuracy_error': 0.4,\r\n            'grasp_success_error': 0.4,\r\n            'execution_time_error': 0.2\r\n        }\r\n\r\n        overall_score = 0\r\n        for metric, weight in weights.items():\r\n            if metric in metrics:\r\n                error = metrics[metric]\r\n                score = max(0, 1 - error)\r\n                overall_score += score * weight\r\n\r\n        metrics['transfer_score'] = overall_score\r\n\r\n        return metrics\r\n\r\n    def _validate_navigation_transfer(self, sim_data, real_data) -> Dict:\r\n        \"\"\"Validate navigation behavior transfer\"\"\"\r\n        metrics = {}\r\n\r\n        # Compare path efficiency\r\n        sim_path_efficiencies = sim_data.get('path_efficiencies', [])  # Actual path / optimal path\r\n        real_path_efficiencies = real_data.get('path_efficiencies', [])\r\n\r\n        if len(sim_path_efficiencies) > 0 and len(real_path_efficiencies) > 0:\r\n            mean_sim_eff = np.mean(sim_path_efficiencies)\r\n            mean_real_eff = np.mean(real_path_efficiencies)\r\n            path_eff_error = abs(mean_real_eff - mean_sim_eff) / (mean_sim_eff + 1e-6)\r\n            metrics['path_efficiency_error'] = path_eff_error\r\n\r\n        # Compare success rates\r\n        sim_success_rates = sim_data.get('success_rates', [])\r\n        real_success_rates = real_data.get('success_rates', [])\r\n\r\n        if len(sim_success_rates) > 0 and len(real_success_rates) > 0:\r\n            mean_sim_success = np.mean(sim_success_rates)\r\n            mean_real_success = np.mean(real_success_rates)\r\n            success_error = abs(mean_real_success - mean_sim_success) / (mean_sim_success + 1e-6)\r\n            metrics['success_rate_error'] = success_error\r\n\r\n        # Compare computation times\r\n        sim_compute_times = sim_data.get('compute_times', [])\r\n        real_compute_times = real_data.get('compute_times', [])\r\n\r\n        if len(sim_compute_times) > 0 and len(real_compute_times) > 0:\r\n            mean_sim_compute = np.mean(sim_compute_times)\r\n            mean_real_compute = np.mean(real_compute_times)\r\n            compute_error = abs(mean_real_compute - mean_sim_compute) / (mean_sim_compute + 1e-6)\r\n            metrics['computation_time_error'] = compute_error\r\n\r\n        # Overall transfer score\r\n        weights = {\r\n            'path_efficiency_error': 0.3,\r\n            'success_rate_error': 0.4,\r\n            'computation_time_error': 0.3\r\n        }\r\n\r\n        overall_score = 0\r\n        for metric, weight in weights.items():\r\n            if metric in metrics:\r\n                error = metrics[metric]\r\n                score = max(0, 1 - error)\r\n                overall_score += score * weight\r\n\r\n        metrics['transfer_score'] = overall_score\r\n\r\n        return metrics\r\n\r\n    def validate_perception_transfer(self, sim_model, real_model,\r\n                                   sim_test_data, real_test_data) -> Dict:\r\n        \"\"\"Validate that perception models transfer from sim to real\"\"\"\r\n\r\n        # Get predictions from both models\r\n        sim_predictions = sim_model.predict(sim_test_data)\r\n        real_predictions = real_model.predict(real_test_data)\r\n\r\n        # Calculate accuracy on respective test sets\r\n        sim_accuracy = accuracy_score(sim_test_data.labels, sim_predictions)\r\n        real_accuracy = accuracy_score(real_test_data.labels, real_predictions)\r\n\r\n        # Calculate domain adaptation metrics\r\n        metrics = {\r\n            'sim_accuracy': sim_accuracy,\r\n            'real_accuracy': real_accuracy,\r\n            'accuracy_gap': abs(sim_accuracy - real_accuracy),\r\n            'transfer_ratio': real_accuracy / (sim_accuracy + 1e-6)\r\n        }\r\n\r\n        # Additional metrics for perception\r\n        if hasattr(sim_test_data, 'features') and hasattr(real_test_data, 'features'):\r\n            # Calculate feature distribution similarity\r\n            from scipy.stats import wasserstein_distance\r\n\r\n            # Flatten features if they're multi-dimensional\r\n            sim_features_flat = sim_test_data.features.reshape(sim_test_data.features.shape[0], -1)\r\n            real_features_flat = real_test_data.features.reshape(real_test_data.features.shape[0], -1)\r\n\r\n            # Calculate Wasserstein distance between feature distributions\r\n            w_distances = []\r\n            for i in range(min(sim_features_flat.shape[1], 10)):  # Limit to first 10 dimensions\r\n                w_dist = wasserstein_distance(\r\n                    sim_features_flat[:, i],\r\n                    real_features_flat[:, i]\r\n                )\r\n                w_distances.append(w_dist)\r\n\r\n            metrics['feature_distribution_similarity'] = 1.0 / (1.0 + np.mean(w_distances))\r\n\r\n        return metrics\r\n\r\n    def analyze_reality_gap(self, sim_data: Dict, real_data: Dict) -> Dict:\r\n        \"\"\"Analyze the reality gap between simulation and real data\"\"\"\r\n\r\n        gap_analysis = {}\r\n\r\n        # Sensor data comparison\r\n        if 'sensor_data' in sim_data and 'sensor_data' in real_data:\r\n            sim_sensors = sim_data['sensor_data']\r\n            real_sensors = real_data['sensor_data']\r\n\r\n            gap_analysis['sensor_gaps'] = {}\r\n\r\n            for sensor_type in set(sim_sensors.keys()).intersection(set(real_sensors.keys())):\r\n                sim_values = np.array(sim_sensors[sensor_type])\r\n                real_values = np.array(real_sensors[sensor_type])\r\n\r\n                # Calculate mean absolute error\r\n                mae = np.mean(np.abs(sim_values - real_values))\r\n\r\n                # Calculate correlation\r\n                if len(sim_values) > 1 and len(real_values) > 1:\r\n                    correlation = np.corrcoef(sim_values.flatten(), real_values.flatten())[0, 1]\r\n                else:\r\n                    correlation = 0\r\n\r\n                gap_analysis['sensor_gaps'][sensor_type] = {\r\n                    'mae': mae,\r\n                    'correlation': correlation,\r\n                    'sim_mean': np.mean(sim_values),\r\n                    'real_mean': np.mean(real_values),\r\n                    'relative_error': mae / (np.mean(np.abs(sim_values)) + 1e-6)\r\n                }\r\n\r\n        # Control response comparison\r\n        if 'control_responses' in sim_data and 'control_responses' in real_data:\r\n            sim_ctrl = sim_data['control_responses']\r\n            real_ctrl = real_data['control_responses']\r\n\r\n            # Calculate control similarity\r\n            ctrl_similarity = self._calculate_control_similarity(sim_ctrl, real_ctrl)\r\n            gap_analysis['control_similarity'] = ctrl_similarity\r\n\r\n        # Environmental interaction comparison\r\n        if 'environmental_data' in sim_data and 'environmental_data' in real_data:\r\n            sim_env = sim_data['environmental_data']\r\n            real_env = real_data['environmental_data']\r\n\r\n            # Analyze environmental interaction differences\r\n            env_gap = self._analyze_environmental_gap(sim_env, real_env)\r\n            gap_analysis['environmental_gap'] = env_gap\r\n\r\n        # Calculate overall reality gap score\r\n        gap_scores = []\r\n        for category, data in gap_analysis.items():\r\n            if isinstance(data, dict) and 'relative_error' in data:\r\n                gap_scores.append(data['relative_error'])\r\n            elif isinstance(data, list):\r\n                # If it's a list of metrics, take the average relative error\r\n                rel_errors = [item.get('relative_error', 0) for item in data if isinstance(item, dict)]\r\n                if rel_errors:\r\n                    gap_scores.extend(rel_errors)\r\n\r\n        gap_analysis['overall_reality_gap'] = np.mean(gap_scores) if gap_scores else 0.0\r\n        gap_analysis['transfer_feasibility'] = max(0, 1 - gap_analysis['overall_reality_gap'])\r\n\r\n        return gap_analysis\r\n\r\n    def _calculate_control_similarity(self, sim_ctrl, real_ctrl) -> Dict:\r\n        \"\"\"Calculate similarity between control responses\"\"\"\r\n        similarities = {}\r\n\r\n        # Compare control signals\r\n        if 'command_signals' in sim_ctrl and 'command_signals' in real_ctrl:\r\n            sim_cmds = np.array(sim_ctrl['command_signals'])\r\n            real_cmds = np.array(real_ctrl['command_signals'])\r\n\r\n            # Calculate RMSE\r\n            rmse = np.sqrt(np.mean((sim_cmds - real_cmds) ** 2))\r\n\r\n            # Calculate correlation\r\n            if len(sim_cmds) > 1 and len(real_cmds) > 1:\r\n                correlation = np.corrcoef(\r\n                    sim_cmds.flatten(),\r\n                    real_cmds.flatten()\r\n                )[0, 1]\r\n            else:\r\n                correlation = 0\r\n\r\n            similarities['command_similarity'] = {\r\n                'rmse': rmse,\r\n                'correlation': correlation,\r\n                'relative_error': rmse / (np.mean(np.abs(sim_cmds)) + 1e-6)\r\n            }\r\n\r\n        # Compare response times\r\n        if 'response_times' in sim_ctrl and 'response_times' in real_ctrl:\r\n            sim_times = np.array(sim_ctrl['response_times'])\r\n            real_times = np.array(real_ctrl['response_times'])\r\n\r\n            time_diff = np.abs(np.mean(real_times) - np.mean(sim_times))\r\n            similarities['timing_similarity'] = {\r\n                'mean_time_diff': time_diff,\r\n                'sim_mean_time': np.mean(sim_times),\r\n                'real_mean_time': np.mean(real_times)\r\n            }\r\n\r\n        return similarities\r\n\r\n    def _analyze_environmental_gap(self, sim_env, real_env) -> Dict:\r\n        \"\"\"Analyze differences in environmental interactions\"\"\"\r\n        env_gap = {}\r\n\r\n        # Compare contact forces\r\n        if 'contact_forces' in sim_env and 'contact_forces' in real_env:\r\n            sim_forces = np.array(sim_env['contact_forces'])\r\n            real_forces = np.array(real_env['contact_forces'])\r\n\r\n            force_error = np.mean(np.abs(sim_forces - real_forces))\r\n            env_gap['contact_force_difference'] = {\r\n                'mean_absolute_error': force_error,\r\n                'sim_mean': np.mean(sim_forces),\r\n                'real_mean': np.mean(real_forces),\r\n                'relative_error': force_error / (np.mean(np.abs(sim_forces)) + 1e-6)\r\n            }\r\n\r\n        # Compare friction effects\r\n        if 'friction_data' in sim_env and 'friction_data' in real_env:\r\n            sim_friction = np.array(sim_env['friction_data'])\r\n            real_friction = np.array(real_env['friction_data'])\r\n\r\n            friction_error = np.mean(np.abs(sim_friction - real_friction))\r\n            env_gap['friction_difference'] = {\r\n                'mean_absolute_error': friction_error,\r\n                'sim_mean': np.mean(sim_friction),\r\n                'real_mean': np.mean(real_friction),\r\n                'relative_error': friction_error / (np.mean(np.abs(sim_friction)) + 1e-6)\r\n            }\r\n\r\n        return env_gap\r\n\r\n    def generate_validation_report(self, validation_results: Dict) -> str:\r\n        \"\"\"Generate a comprehensive validation report\"\"\"\r\n\r\n        report = []\r\n        report.append(\"# Sim-to-Real Transfer Validation Report\\n\")\r\n        report.append(f\"**Generated**: {np.datetime64('now')}\\n\")\r\n\r\n        # Overall transfer score\r\n        if 'transfer_score' in validation_results:\r\n            score = validation_results['transfer_score']\r\n            report.append(f\"## Overall Transfer Score: {score:.3f}\")\r\n            report.append(f\"**Transfer Quality**: {self._interpret_transfer_score(score)}\\n\")\r\n\r\n        # Detailed metrics\r\n        report.append(\"## Detailed Analysis\\n\")\r\n\r\n        for key, value in validation_results.items():\r\n            if isinstance(value, dict):\r\n                report.append(f\"### {key.replace('_', ' ').title()}\")\r\n                for sub_key, sub_value in value.items():\r\n                    if isinstance(sub_value, (int, float)):\r\n                        report.append(f\"- {sub_key.replace('_', ' ').title()}: {sub_value:.4f}\")\r\n                    else:\r\n                        report.append(f\"- {sub_key.replace('_', ' ').title()}: {sub_value}\")\r\n                report.append(\"\")\r\n            elif isinstance(value, (int, float)):\r\n                report.append(f\"- {key.replace('_', ' ').title()}: {value:.4f}\")\r\n\r\n        # Recommendations\r\n        report.append(\"## Recommendations\\n\")\r\n        score = validation_results.get('transfer_score', 0)\r\n\r\n        if score > 0.8:\r\n            report.append(\"- Transfer quality is excellent, ready for deployment\")\r\n        elif score > 0.6:\r\n            report.append(\"- Transfer quality is good, minor adjustments may be needed\")\r\n        elif score > 0.4:\r\n            report.append(\"- Transfer quality is moderate, significant calibration required\")\r\n        else:\r\n            report.append(\"- Transfer quality is poor, extensive domain adaptation needed\")\r\n\r\n        # Improvement suggestions\r\n        reality_gap = validation_results.get('reality_gap_analysis', {}).get('overall_reality_gap', 1.0)\r\n        if reality_gap > 0.5:\r\n            report.append(\"\\n### Suggested Improvements:\")\r\n            report.append(\"- Increase domain randomization in simulation\")\r\n            report.append(\"- Improve physics simulation accuracy\")\r\n            report.append(\"- Calibrate sensor models to match real hardware\")\r\n            report.append(\"- Implement online adaptation techniques\")\r\n\r\n        return \"\\n\".join(report)\r\n\r\n    def _interpret_transfer_score(self, score: float) -> str:\r\n        \"\"\"Interpret transfer score quality\"\"\"\r\n        if score > 0.8:\r\n            return \"Excellent\"\r\n        elif score > 0.6:\r\n            return \"Good\"\r\n        elif score > 0.4:\r\n            return \"Fair\"\r\n        else:\r\n            return \"Poor\"\r\n\r\nclass RealityGapAnalyzer:\r\n    \"\"\"Analyze and quantify the reality gap\"\"\"\r\n\r\n    def __init__(self):\r\n        self.gap_metrics = {}\r\n\r\n    def calculate_reality_gap(self, sim_data: np.ndarray, real_data: np.ndarray) -> Dict:\r\n        \"\"\"Calculate various metrics for reality gap quantification\"\"\"\r\n\r\n        gap_metrics = {}\r\n\r\n        # Statistical similarity measures\r\n        gap_metrics['mean_difference'] = np.mean(np.abs(sim_data - real_data))\r\n        gap_metrics['std_ratio'] = np.std(real_data) / (np.std(sim_data) + 1e-6)\r\n        gap_metrics['correlation'] = np.corrcoef(sim_data.flatten(), real_data.flatten())[0, 1]\r\n\r\n        # Distribution similarity (using KL divergence approximation)\r\n        sim_hist, _ = np.histogram(sim_data, bins=50, density=True)\r\n        real_hist, _ = np.histogram(real_data, bins=50, density=True)\r\n\r\n        # Normalize histograms\r\n        sim_hist = sim_hist / (np.sum(sim_hist) + 1e-6)\r\n        real_hist = real_hist / (np.sum(real_hist) + 1e-6)\r\n\r\n        # Calculate Jensen-Shannon divergence (symmetric version of KL)\r\n        m = 0.5 * (sim_hist + real_hist)\r\n        js_div = 0.5 * np.sum(sim_hist * np.log((sim_hist + 1e-6) / (m + 1e-6))) + \\\r\n                 0.5 * np.sum(real_hist * np.log((real_hist + 1e-6) / (m + 1e-6)))\r\n\r\n        gap_metrics['js_divergence'] = js_div\r\n\r\n        # Frequency domain analysis\r\n        sim_fft = np.fft.fft(sim_data)\r\n        real_fft = np.fft.fft(real_data)\r\n        freq_similarity = np.mean(np.abs(sim_fft - real_fft)) / (np.mean(np.abs(sim_fft)) + 1e-6)\r\n        gap_metrics['frequency_difference'] = freq_similarity\r\n\r\n        # Overall gap score (weighted combination)\r\n        weights = {\r\n            'mean_difference': 0.3,\r\n            'std_ratio_deviation': 0.2,  # How much std ratio deviates from 1.0\r\n            'correlation_inverse': 0.3,   # 1 - correlation\r\n            'js_divergence': 0.2\r\n        }\r\n\r\n        std_ratio_dev = abs(gap_metrics['std_ratio'] - 1.0)\r\n        correlation_inverse = 1.0 - abs(gap_metrics['correlation'])  # Inverse of correlation\r\n\r\n        overall_gap = (\r\n            weights['mean_difference'] * (gap_metrics['mean_difference'] / (np.mean(np.abs(sim_data)) + 1e-6)) +\r\n            weights['std_ratio_deviation'] * std_ratio_dev +\r\n            weights['correlation_inverse'] * correlation_inverse +\r\n            weights['js_divergence'] * gap_metrics['js_divergence']\r\n        )\r\n\r\n        gap_metrics['overall_reality_gap'] = overall_gap\r\n        gap_metrics['transfer_feasibility'] = max(0, 1 - overall_gap)\r\n\r\n        return gap_metrics\r\n\r\n    def visualize_reality_gap(self, sim_data: np.ndarray, real_data: np.ndarray,\r\n                            title: str = \"Reality Gap Analysis\"):\r\n        \"\"\"Create visualization of reality gap\"\"\"\r\n\r\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\r\n        fig.suptitle(title, fontsize=16)\r\n\r\n        # Time series comparison\r\n        axes[0, 0].plot(sim_data[:1000], label='Simulation', alpha=0.7)\r\n        axes[0, 0].plot(real_data[:1000], label='Real', alpha=0.7)\r\n        axes[0, 0].set_title('Time Series Comparison')\r\n        axes[0, 0].legend()\r\n        axes[0, 0].grid(True, alpha=0.3)\r\n\r\n        # Histogram comparison\r\n        axes[0, 1].hist(sim_data, bins=50, alpha=0.5, label='Simulation', density=True)\r\n        axes[0, 1].hist(real_data, bins=50, alpha=0.5, label='Real', density=True)\r\n        axes[0, 1].set_title('Distribution Comparison')\r\n        axes[0, 1].legend()\r\n        axes[0, 1].grid(True, alpha=0.3)\r\n\r\n        # Scatter plot\r\n        min_len = min(len(sim_data), len(real_data))\r\n        axes[1, 0].scatter(sim_data[:min_len], real_data[:min_len], alpha=0.5)\r\n        axes[1, 0].plot([sim_data.min(), sim_data.max()], [sim_data.min(), sim_data.max()], 'r--', lw=2)\r\n        axes[1, 0].set_xlabel('Simulation Values')\r\n        axes[1, 0].set_ylabel('Real Values')\r\n        axes[1, 0].set_title('Scatter Plot (Perfect = Diagonal)')\r\n        axes[1, 0].grid(True, alpha=0.3)\r\n\r\n        # Power spectral density comparison\r\n        f_sim, psd_sim = plt.mlab.psd(sim_data)\r\n        f_real, psd_real = plt.mlab.psd(real_data)\r\n        axes[1, 1].semilogy(f_sim, psd_sim, label='Simulation', alpha=0.7)\r\n        axes[1, 1].semilogy(f_real, psd_real, label='Real', alpha=0.7)\r\n        axes[1, 1].set_title('Power Spectral Density')\r\n        axes[1, 1].set_xlabel('Frequency')\r\n        axes[1, 1].set_ylabel('PSD')\r\n        axes[1, 1].legend()\r\n        axes[1, 1].grid(True, alpha=0.3)\r\n\r\n        plt.tight_layout()\r\n        return fig\r\n\r\ndef main_validation_example():\r\n    \"\"\"Example of how to use the validation framework\"\"\"\r\n\r\n    # Initialize validators\r\n    transfer_validator = TransferValidator()\r\n    reality_analyzer = RealityGapAnalyzer()\r\n\r\n    # Example: Validate locomotion behavior transfer\r\n    sim_locomotion_data = {\r\n        'stride_lengths': np.random.normal(0.6, 0.1, 100),\r\n        'step_frequencies': np.random.normal(1.8, 0.2, 100),\r\n        'com_variances': np.random.normal(0.02, 0.005, 100)\r\n    }\r\n\r\n    real_locomotion_data = {\r\n        'stride_lengths': np.random.normal(0.58, 0.12, 100),  # Slightly different\r\n        'step_frequencies': np.random.normal(1.75, 0.25, 100),  # Slightly different\r\n        'com_variances': np.random.normal(0.025, 0.008, 100)   # Slightly different\r\n    }\r\n\r\n    # Validate behavior transfer\r\n    locomotion_metrics = transfer_validator.validate_behavior_transfer(\r\n        sim_locomotion_data, real_locomotion_data, 'locomotion'\r\n    )\r\n\r\n    print(\"Locomotion Transfer Metrics:\")\r\n    for key, value in locomotion_metrics.items():\r\n        if isinstance(value, float):\r\n            print(f\"  {key}: {value:.4f}\")\r\n        else:\r\n            print(f\"  {key}: {value}\")\r\n\r\n    # Example: Analyze reality gap\r\n    sim_sensor_data = np.random.normal(0, 1, 1000)\r\n    real_sensor_data = np.random.normal(0.1, 1.1, 1000)  # Slightly shifted\r\n\r\n    gap_metrics = reality_analyzer.calculate_reality_gap(sim_sensor_data, real_sensor_data)\r\n\r\n    print(\"\\nReality Gap Metrics:\")\r\n    for key, value in gap_metrics.items():\r\n        if isinstance(value, float):\r\n            print(f\"  {key}: {value:.4f}\")\r\n\r\n    # Generate validation report\r\n    validation_results = {\r\n        'locomotion_transfer': locomotion_metrics,\r\n        'reality_gap_analysis': gap_metrics\r\n    }\r\n\r\n    report = transfer_validator.generate_validation_report(validation_results)\r\n    print(\"\\nValidation Report:\")\r\n    print(report)\r\n\r\nif __name__ == \"__main__\":\r\n    main_validation_example()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"adaptive-parameter-tuning",children:"Adaptive Parameter Tuning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nfrom scipy.optimize import minimize\r\nfrom typing import Dict, Callable, Any\r\nimport time\r\n\r\nclass AdaptiveParameterTuner:\r\n    """Adaptively tune parameters for sim-to-real transfer"""\r\n\r\n    def __init__(self, parameter_bounds: Dict[str, tuple],\r\n                 performance_metric: Callable):\r\n        self.parameter_bounds = parameter_bounds\r\n        self.performance_metric = performance_metric\r\n        self.history = {\r\n            \'parameters\': [],\r\n            \'performance\': [],\r\n            \'timestamps\': []\r\n        }\r\n\r\n    def bayesian_optimization_tune(self, initial_params: Dict[str, float],\r\n                                  max_evaluations: int = 50) -> Dict[str, float]:\r\n        """Use Bayesian optimization for parameter tuning"""\r\n\r\n        def objective(params_array):\r\n            """Objective function for optimization (negative performance for minimization)"""\r\n            # Convert array back to parameter dictionary\r\n            params = {}\r\n            param_names = list(self.parameter_bounds.keys())\r\n            for i, name in enumerate(param_names):\r\n                params[name] = params_array[i]\r\n\r\n            # Evaluate performance\r\n            performance = self.performance_metric(params)\r\n\r\n            # Store history\r\n            self.history[\'parameters\'].append(params.copy())\r\n            self.history[\'performance\'].append(performance)\r\n            self.history[\'timestamps\'].append(time.time())\r\n\r\n            # Return negative performance (since optimizer minimizes)\r\n            return -performance\r\n\r\n        # Initial parameters as array\r\n        initial_array = np.array([initial_params[name] for name in self.parameter_bounds.keys()])\r\n\r\n        # Bounds as list of tuples\r\n        bounds_list = [self.parameter_bounds[name] for name in self.parameter_bounds.keys()]\r\n\r\n        # Perform optimization\r\n        result = minimize(\r\n            objective,\r\n            initial_array,\r\n            method=\'L-BFGS-B\',\r\n            bounds=bounds_list,\r\n            options={\'maxiter\': max_evaluations}\r\n        )\r\n\r\n        # Convert result back to parameter dictionary\r\n        optimal_params = {}\r\n        param_names = list(self.parameter_bounds.keys())\r\n        for i, name in enumerate(param_names):\r\n            optimal_params[name] = result.x[i]\r\n\r\n        return optimal_params\r\n\r\n    def evolutionary_tune(self, population_size: int = 20, generations: int = 30) -> Dict[str, float]:\r\n        """Use evolutionary algorithm for parameter tuning"""\r\n\r\n        # Initialize population\r\n        param_names = list(self.parameter_bounds.keys())\r\n        population = []\r\n\r\n        for _ in range(population_size):\r\n            individual = {}\r\n            for name in param_names:\r\n                min_val, max_val = self.parameter_bounds[name]\r\n                individual[name] = np.random.uniform(min_val, max_val)\r\n            population.append(individual)\r\n\r\n        # Evolution loop\r\n        for gen in range(generations):\r\n            # Evaluate fitness\r\n            fitness_scores = []\r\n            for individual in population:\r\n                performance = self.performance_metric(individual)\r\n                fitness_scores.append(performance)\r\n\r\n                # Store in history\r\n                self.history[\'parameters\'].append(individual.copy())\r\n                self.history[\'performance\'].append(performance)\r\n                self.history[\'timestamps\'].append(time.time())\r\n\r\n            # Select parents (tournament selection)\r\n            selected_indices = []\r\n            for _ in range(population_size):\r\n                tournament_size = 3\r\n                tournament_indices = np.random.choice(len(population), tournament_size)\r\n                tournament_fitness = [fitness_scores[i] for i in tournament_indices]\r\n                winner_idx = tournament_indices[np.argmax(tournament_fitness)]\r\n                selected_indices.append(winner_idx)\r\n\r\n            # Create new population through crossover and mutation\r\n            new_population = []\r\n            for i in range(0, population_size, 2):\r\n                parent1 = population[selected_indices[i]]\r\n                parent2 = population[selected_indices[i + 1]] if i + 1 < len(selected_indices) else population[selected_indices[0]]\r\n\r\n                # Crossover\r\n                child1, child2 = self._crossover(parent1, parent2)\r\n\r\n                # Mutation\r\n                child1 = self._mutate(child1)\r\n                child2 = self._mutate(child2)\r\n\r\n                new_population.extend([child1, child2])\r\n\r\n            population = new_population[:population_size]\r\n\r\n        # Return best individual\r\n        final_fitness = []\r\n        for individual in population:\r\n            final_fitness.append(self.performance_metric(individual))\r\n\r\n        best_idx = np.argmax(final_fitness)\r\n        return population[best_idx]\r\n\r\n    def _crossover(self, parent1: Dict[str, float], parent2: Dict[str, float]) -> tuple:\r\n        """Perform crossover between two parents"""\r\n        child1, child2 = {}, {}\r\n        param_names = list(self.parameter_bounds.keys())\r\n\r\n        for name in param_names:\r\n            # Blend values\r\n            alpha = np.random.uniform(0.3, 0.7)\r\n            child1[name] = alpha * parent1[name] + (1 - alpha) * parent2[name]\r\n            child2[name] = (1 - alpha) * parent1[name] + alpha * parent2[name]\r\n\r\n            # Ensure bounds\r\n            min_val, max_val = self.parameter_bounds[name]\r\n            child1[name] = np.clip(child1[name], min_val, max_val)\r\n            child2[name] = np.clip(child2[name], min_val, max_val)\r\n\r\n        return child1, child2\r\n\r\n    def _mutate(self, individual: Dict[str, float], mutation_rate: float = 0.1) -> Dict[str, float]:\r\n        """Apply mutation to an individual"""\r\n        mutated = individual.copy()\r\n        param_names = list(self.parameter_bounds.keys())\r\n\r\n        for name in param_names:\r\n            if np.random.random() < mutation_rate:\r\n                min_val, max_val = self.parameter_bounds[name]\r\n                # Add Gaussian noise\r\n                noise = np.random.normal(0, (max_val - min_val) * 0.1)\r\n                mutated[name] = np.clip(individual[name] + noise, min_val, max_val)\r\n\r\n        return mutated\r\n\r\n    def get_tuning_insights(self) -> Dict[str, Any]:\r\n        """Get insights from the tuning process"""\r\n\r\n        if not self.history[\'performance\']:\r\n            return {}\r\n\r\n        insights = {\r\n            \'best_performance\': max(self.history[\'performance\']),\r\n            \'worst_performance\': min(self.history[\'performance\']),\r\n            \'average_performance\': np.mean(self.history[\'performance\']),\r\n            \'performance_std\': np.std(self.history[\'performance\']),\r\n            \'tuning_stability\': self._calculate_tuning_stability(),\r\n            \'convergence_rate\': self._calculate_convergence_rate()\r\n        }\r\n\r\n        return insights\r\n\r\n    def _calculate_tuning_stability(self) -> float:\r\n        """Calculate how stable the tuning process was"""\r\n        if len(self.history[\'performance\']) < 2:\r\n            return 0.0\r\n\r\n        # Measure stability as inverse of performance volatility\r\n        performance_changes = np.diff(self.history[\'performance\'])\r\n        volatility = np.std(performance_changes)\r\n\r\n        # Normalize to [0, 1] where 1 is most stable\r\n        return max(0, 1 - volatility)\r\n\r\n    def _calculate_convergence_rate(self) -> float:\r\n        """Calculate how quickly tuning converged to good parameters"""\r\n        if len(self.history[\'performance\']) < 10:\r\n            return 0.0\r\n\r\n        # Calculate improvement in first half vs second half\r\n        mid_point = len(self.history[\'performance\']) // 2\r\n        first_half_best = max(self.history[\'performance\'][:mid_point])\r\n        second_half_best = max(self.history[\'performance\'][mid_point:])\r\n\r\n        # If second half has better performance, it\'s converging\r\n        if second_half_best > first_half_best:\r\n            improvement = (second_half_best - first_half_best) / (first_half_best + 1e-6)\r\n            return min(1.0, improvement * 10)  # Scale and cap at 1.0\r\n        else:\r\n            return 0.0\r\n\r\nclass OnlineAdaptationSystem:\r\n    """Online adaptation system for continuous parameter adjustment"""\r\n\r\n    def __init__(self, base_params: Dict[str, float], adaptation_rate: float = 0.01):\r\n        self.base_params = base_params.copy()\r\n        self.current_params = base_params.copy()\r\n        self.adaptation_rate = adaptation_rate\r\n        self.performance_history = []\r\n        self.param_history = []\r\n\r\n    def update_parameters(self, current_performance: float,\r\n                         reference_performance: float = 1.0) -> Dict[str, float]:\r\n        """Update parameters based on performance feedback"""\r\n\r\n        # Store performance\r\n        self.performance_history.append(current_performance)\r\n\r\n        # Calculate performance ratio\r\n        perf_ratio = current_performance / (reference_performance + 1e-6)\r\n\r\n        # Adaptive adjustment based on performance\r\n        if perf_ratio < 0.8:  # Performance is poor\r\n            # Aggressive adaptation\r\n            adaptation_factor = min(1.0, (1.0 - perf_ratio) * 2.0)\r\n        elif perf_ratio < 0.95:  # Performance is acceptable but not great\r\n            # Moderate adaptation\r\n            adaptation_factor = (1.0 - perf_ratio) * 1.0\r\n        else:  # Performance is good\r\n            # Conservative adaptation\r\n            adaptation_factor = max(0.1, (1.0 - perf_ratio) * 0.5)\r\n\r\n        # Adjust parameters based on performance\r\n        for param_name, base_value in self.base_params.items():\r\n            # Calculate gradient based on recent performance changes\r\n            if len(self.performance_history) > 1:\r\n                recent_perf_change = (\r\n                    self.performance_history[-1] -\r\n                    self.performance_history[-min(5, len(self.performance_history))]\r\n                ) / max(1, len(self.performance_history) - 1)\r\n\r\n                # Adjust parameter in direction that improves performance\r\n                adjustment = self.adaptation_rate * adaptation_factor * recent_perf_change\r\n                new_value = base_value + adjustment\r\n\r\n                # Apply bounds (assume reasonable bounds based on parameter type)\r\n                if param_name.endswith(\'_gain\') or param_name.endswith(\'_weight\'):\r\n                    # For gains and weights, constrain to positive values\r\n                    new_value = max(0.01, min(10.0, new_value))\r\n                elif param_name.endswith(\'_threshold\'):\r\n                    # For thresholds, constrain to reasonable range\r\n                    new_value = max(0.001, min(1.0, new_value))\r\n                else:\r\n                    # For other parameters, allow more flexibility\r\n                    new_value = max(-10.0, min(10.0, new_value))\r\n\r\n                self.current_params[param_name] = new_value\r\n\r\n        # Store current parameters\r\n        self.param_history.append(self.current_params.copy())\r\n\r\n        return self.current_params.copy()\r\n\r\n    def get_adaptation_metrics(self) -> Dict[str, float]:\r\n        """Get metrics about the adaptation process"""\r\n\r\n        if len(self.performance_history) < 2:\r\n            return {}\r\n\r\n        # Calculate performance trend\r\n        recent_perf = self.performance_history[-min(10, len(self.performance_history)):]\r\n        perf_trend = np.polyfit(range(len(recent_perf)), recent_perf, 1)[0] if len(recent_perf) > 1 else 0\r\n\r\n        # Calculate parameter stability\r\n        if len(self.param_history) > 1:\r\n            param_changes = []\r\n            for i in range(1, len(self.param_history)):\r\n                for param_name in self.base_params.keys():\r\n                    change = abs(\r\n                        self.param_history[i][param_name] -\r\n                        self.param_history[i-1][param_name]\r\n                    ) / (abs(self.base_params[param_name]) + 1e-6)\r\n                    param_changes.append(change)\r\n\r\n            avg_param_change = np.mean(param_changes) if param_changes else 0\r\n        else:\r\n            avg_param_change = 0\r\n\r\n        metrics = {\r\n            \'performance_trend\': perf_trend,\r\n            \'average_param_change\': avg_param_change,\r\n            \'adaptation_stability\': max(0, 1 - avg_param_change * 10),  # Inverse of parameter change\r\n            \'current_performance\': self.performance_history[-1] if self.performance_history else 0,\r\n            \'best_performance\': max(self.performance_history) if self.performance_history else 0\r\n        }\r\n\r\n        return metrics\r\n\r\ndef example_performance_metric(params: Dict[str, float]) -> float:\r\n    """Example performance metric function"""\r\n    # Simulate a performance function that has an optimum\r\n    # This is just an example - in reality, this would involve running the actual system\r\n    x = params.get(\'param1\', 0.5)\r\n    y = params.get(\'param2\', 0.3)\r\n\r\n    # Example function with optimum at (0.5, 0.3)\r\n    performance = 1.0 - ((x - 0.5)**2 + (y - 0.3)**2) * 10\r\n    return max(0, min(1, performance))  # Clamp to [0, 1]\r\n\r\ndef main_optimization_example():\r\n    """Example of parameter optimization"""\r\n\r\n    # Define parameter bounds\r\n    param_bounds = {\r\n        \'param1\': (0.0, 1.0),\r\n        \'param2\': (0.0, 1.0),\r\n        \'param3\': (0.1, 2.0)\r\n    }\r\n\r\n    # Initialize tuner\r\n    tuner = AdaptiveParameterTuner(param_bounds, example_performance_metric)\r\n\r\n    # Initial parameters\r\n    initial_params = {\r\n        \'param1\': 0.3,\r\n        \'param2\': 0.4,\r\n        \'param3\': 1.0\r\n    }\r\n\r\n    # Bayesian optimization\r\n    print("Starting Bayesian optimization...")\r\n    optimal_params = tuner.bayesian_optimization_tune(initial_params, max_evaluations=20)\r\n\r\n    print(f"Optimal parameters: {optimal_params}")\r\n\r\n    # Get insights\r\n    insights = tuner.get_tuning_insights()\r\n    print(f"Tuning insights: {insights}")\r\n\r\n    # Example of online adaptation\r\n    print("\\nStarting online adaptation...")\r\n    online_adapter = OnlineAdaptationSystem(initial_params, adaptation_rate=0.01)\r\n\r\n    # Simulate performance feedback loop\r\n    for step in range(20):\r\n        # Simulate current performance (would come from real system)\r\n        current_performance = example_performance_metric(online_adapter.current_params)\r\n\r\n        # Update parameters based on performance\r\n        updated_params = online_adapter.update_parameters(current_performance)\r\n\r\n        if step % 5 == 0:\r\n            print(f"Step {step}: Performance = {current_performance:.4f}, Params = {updated_params}")\r\n\r\n    # Get adaptation metrics\r\n    metrics = online_adapter.get_adaptation_metrics()\r\n    print(f"\\nAdaptation metrics: {metrics}")\r\n\r\nif __name__ == "__main__":\r\n    main_optimization_example()\n'})})]})}function d(r={}){const{wrapper:n}={...(0,i.R)(),...r.components};return n?(0,t.jsx)(n,{...r,children:(0,t.jsx)(_,{...r})}):_(r)}},8453(r,n,e){e.d(n,{R:()=>s,x:()=>o});var a=e(6540);const t={},i=a.createContext(t);function s(r){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof r?r(n):{...n,...r}},[n,r])}function o(r){let n;return n=r.disableParentContext?"function"==typeof r.components?r.components(t):r.components||t:s(r.components),a.createElement(i.Provider,{value:n},r.children)}}}]);