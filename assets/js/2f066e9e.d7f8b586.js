"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[764],{4813(n,e,r){r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-3/synthetic-data","title":"Synthetic Data Generation","description":"Comprehensive guide to generating synthetic training data for robotics AI models","source":"@site/docs/module-3/synthetic-data.md","sourceDirName":"module-3","slug":"/module-3/synthetic-data","permalink":"/physical-ai/docs/module-3/synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-textbook/book-ai/tree/main/docs/module-3/synthetic-data.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"synthetic-data","title":"Synthetic Data Generation","sidebar_position":4,"description":"Comprehensive guide to generating synthetic training data for robotics AI models","keywords":["synthetic data","training datasets","computer vision","deep learning","domain randomization","data augmentation"]},"sidebar":"textbook","previous":{"title":"Isaac Sim","permalink":"/physical-ai/docs/module-3/isaac-sim"},"next":{"title":"Isaac ROS VSLAM","permalink":"/physical-ai/docs/module-3/isaac-vslam"}}');var t=r(4848),i=r(8453);const s={id:"synthetic-data",title:"Synthetic Data Generation",sidebar_position:4,description:"Comprehensive guide to generating synthetic training data for robotics AI models",keywords:["synthetic data","training datasets","computer vision","deep learning","domain randomization","data augmentation"]},o="Synthetic Data Generation",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Synthetic Data",id:"introduction-to-synthetic-data",level:2},{value:"Why Synthetic Data?",id:"why-synthetic-data",level:3},{value:"Data Scarcity",id:"data-scarcity",level:4},{value:"Data Diversity",id:"data-diversity",level:4},{value:"Annotation Quality",id:"annotation-quality",level:4},{value:"Benefits of Synthetic Data",id:"benefits-of-synthetic-data",level:3},{value:"Cost Efficiency",id:"cost-efficiency",level:4},{value:"Controlled Environments",id:"controlled-environments",level:4},{value:"Safety",id:"safety",level:4},{value:"Scalability",id:"scalability",level:4},{value:"Synthetic Data Generation Pipeline",id:"synthetic-data-generation-pipeline",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Scene Setup",id:"scene-setup",level:3},{value:"Domain Randomization Techniques",id:"domain-randomization-techniques",level:2},{value:"Understanding Domain Randomization",id:"understanding-domain-randomization",level:3},{value:"Multi-Modal Data Generation",id:"multi-modal-data-generation",level:2},{value:"RGB Data Capture",id:"rgb-data-capture",level:3},{value:"Depth Data Capture",id:"depth-data-capture",level:3},{value:"Semantic Segmentation Capture",id:"semantic-segmentation-capture",level:3},{value:"Data Augmentation Techniques",id:"data-augmentation-techniques",level:2},{value:"Geometric Transformations",id:"geometric-transformations",level:3},{value:"Photometric Transformations",id:"photometric-transformations",level:3},{value:"Data Quality Validation",id:"data-quality-validation",level:2},{value:"Synthetic Data Quality Metrics",id:"synthetic-data-quality-metrics",level:3},{value:"Dataset Organization and Management",id:"dataset-organization-and-management",level:2},{value:"Dataset Structure",id:"dataset-structure",level:3},{value:"Learning Objectives Review",id:"learning-objectives-review",level:2},{value:"Practical Exercise",id:"practical-exercise",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"synthetic-data-generation",children:"Synthetic Data Generation"})}),"\n",(0,t.jsx)(e.p,{children:"Synthetic data generation is a critical component of modern AI development in robotics. This chapter covers techniques for creating high-quality synthetic datasets that can be used to train AI models for robotics applications, leveraging simulation environments like Isaac Sim."}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the importance and benefits of synthetic data in robotics"}),"\n",(0,t.jsx)(e.li,{children:"Learn techniques for generating diverse synthetic datasets"}),"\n",(0,t.jsx)(e.li,{children:"Implement domain randomization strategies for robust training"}),"\n",(0,t.jsx)(e.li,{children:"Create multi-modal synthetic data (RGB, depth, segmentation)"}),"\n",(0,t.jsx)(e.li,{children:"Apply data augmentation techniques for synthetic datasets"}),"\n",(0,t.jsx)(e.li,{children:"Validate synthetic data quality and effectiveness"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"introduction-to-synthetic-data",children:"Introduction to Synthetic Data"}),"\n",(0,t.jsx)(e.h3,{id:"why-synthetic-data",children:"Why Synthetic Data?"}),"\n",(0,t.jsx)(e.p,{children:"Synthetic data generation addresses several challenges in robotics AI development:"}),"\n",(0,t.jsx)(e.h4,{id:"data-scarcity",children:"Data Scarcity"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Real-world data collection is expensive and time-consuming"}),"\n",(0,t.jsx)(e.li,{children:"Rare scenarios difficult to capture in real data"}),"\n",(0,t.jsx)(e.li,{children:"Safety concerns limit data collection in dangerous environments"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"data-diversity",children:"Data Diversity"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Real-world data may not cover all edge cases"}),"\n",(0,t.jsx)(e.li,{children:"Weather and lighting conditions vary"}),"\n",(0,t.jsx)(e.li,{children:"Object appearances and arrangements limited"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"annotation-quality",children:"Annotation Quality"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Manual annotation is labor-intensive and error-prone"}),"\n",(0,t.jsx)(e.li,{children:"3D annotations particularly challenging"}),"\n",(0,t.jsx)(e.li,{children:"Consistent labeling across datasets"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"benefits-of-synthetic-data",children:"Benefits of Synthetic Data"}),"\n",(0,t.jsx)(e.h4,{id:"cost-efficiency",children:"Cost Efficiency"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Eliminate expensive data collection campaigns"}),"\n",(0,t.jsx)(e.li,{children:"Reduce annotation costs significantly"}),"\n",(0,t.jsx)(e.li,{children:"Enable rapid dataset generation"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"controlled-environments",children:"Controlled Environments"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Precise control over scene parameters"}),"\n",(0,t.jsx)(e.li,{children:"Perfect ground truth annotations"}),"\n",(0,t.jsx)(e.li,{children:"Repeatable experiments"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"safety",children:"Safety"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Train in dangerous scenarios without risk"}),"\n",(0,t.jsx)(e.li,{children:"Test edge cases safely"}),"\n",(0,t.jsx)(e.li,{children:"Validate before real-world deployment"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"scalability",children:"Scalability"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Generate unlimited data variations"}),"\n",(0,t.jsx)(e.li,{children:"Parallel data generation"}),"\n",(0,t.jsx)(e.li,{children:"Automated annotation processes"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"synthetic-data-generation-pipeline",children:"Synthetic Data Generation Pipeline"}),"\n",(0,t.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,t.jsx)(e.p,{children:"The synthetic data generation pipeline consists of several key components:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Scene Setup   \u2502\u2500\u2500\u2500\u25b6\u2502   Randomization  \u2502\u2500\u2500\u2500\u25b6\u2502   Data Capture  \u2502\r\n\u2502   (Objects,     \u2502    \u2502   (Domain,      \u2502    \u2502   (Cameras,     \u2502\r\n\u2502   Lighting,     \u2502    \u2502   Materials)     \u2502    \u2502   Sensors)      \u2502\r\n\u2502   Environment)  \u2502    \u2502                  \u2502    \u2502                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502                       \u2502                       \u2502\r\n         \u25bc                       \u25bc                       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   Annotation    \u2502\u2500\u2500\u2500\u25b6\u2502   Post-Processing\u2502\u2500\u2500\u2500\u25b6\u2502   Dataset       \u2502\r\n\u2502   Generation    \u2502    \u2502   (Augmentation,\u2502    \u2502   Storage       \u2502\r\n\u2502   (Labels,      \u2502    \u2502   Filtering)    \u2502    \u2502                 \u2502\r\n\u2502   Metadata)     \u2502    \u2502                  \u2502    \u2502                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(e.h3,{id:"scene-setup",children:"Scene Setup"}),"\n",(0,t.jsx)(e.p,{children:"Creating realistic and diverse scenes is the foundation of good synthetic data:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.prims import get_prim_at_path, create_primitive\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.utils.carb import carb_settings_path\r\nimport numpy as np\r\nimport random\r\nimport json\r\n\r\nclass SceneSetup:\r\n    def __init__(self, world):\r\n        self.world = world\r\n        self.scene_objects = []\r\n        self.lighting_config = {}\r\n        self.materials_library = []\r\n\r\n    def setup_basic_environment(self):\r\n        \"\"\"Setup basic environment components\"\"\"\r\n        # Add ground plane\r\n        self.world.scene.add_default_ground_plane(\r\n            prim_path=\"/World/groundPlane\",\r\n            name=\"ground_plane\",\r\n            size=10.0\r\n        )\r\n\r\n        # Add sky dome\r\n        sky_dome = self.world.scene.add(\r\n            omni.isaac.core.objects.DomeLight(\r\n                prim_path=\"/World/SkyDome\",\r\n                name=\"sky_dome\",\r\n                intensity=3000,\r\n                color=(0.5, 0.6, 1.0)\r\n            )\r\n        )\r\n\r\n        return sky_dome\r\n\r\n    def add_objects_to_scene(self, object_configs):\r\n        \"\"\"Add objects to the scene based on configuration\"\"\"\r\n        for config in object_configs:\r\n            obj_type = config.get('type', 'cube')\r\n            position = config.get('position', [0, 0, 0])\r\n            scale = config.get('scale', [1, 1, 1])\r\n            color = config.get('color', [0.8, 0.2, 0.2])\r\n\r\n            if obj_type == 'cube':\r\n                obj = self.world.scene.add(\r\n                    omni.isaac.core.objects.DynamicCuboid(\r\n                        prim_path=f\"/World/Objects/Object_{len(self.scene_objects)}\",\r\n                        name=f\"object_{len(self.scene_objects)}\",\r\n                        position=position,\r\n                        size=0.2,\r\n                        color=color\r\n                    )\r\n                )\r\n            elif obj_type == 'sphere':\r\n                obj = self.world.scene.add(\r\n                    omni.isaac.core.objects.DynamicSphere(\r\n                        prim_path=f\"/World/Objects/Object_{len(self.scene_objects)}\",\r\n                        name=f\"object_{len(self.scene_objects)}\",\r\n                        position=position,\r\n                        radius=0.1,\r\n                        color=color\r\n                    )\r\n                )\r\n            elif obj_type == 'cylinder':\r\n                obj = self.world.scene.add(\r\n                    omni.isaac.core.objects.DynamicCylinder(\r\n                        prim_path=f\"/World/Objects/Object_{len(self.scene_objects)}\",\r\n                        name=f\"object_{len(self.scene_objects)}\",\r\n                        position=position,\r\n                        radius=0.1,\r\n                        height=0.2,\r\n                        color=color\r\n                    )\r\n                )\r\n\r\n            self.scene_objects.append(obj)\r\n\r\n    def setup_camera_rigs(self, camera_configs):\r\n        \"\"\"Setup camera configurations for data capture\"\"\"\r\n        cameras = []\r\n\r\n        for i, config in enumerate(camera_configs):\r\n            camera = omni.isaac.sensor.Camera(\r\n                prim_path=f\"/World/Cameras/Camera_{i}\",\r\n                name=f\"camera_{i}\",\r\n                position=config['position'],\r\n                look_at=config['look_at'],\r\n                resolution=config['resolution']\r\n            )\r\n\r\n            # Set camera parameters\r\n            camera.set_focal_length(config.get('focal_length', 24.0))\r\n            camera.set_horizontal_aperture(config.get('horizontal_aperture', 20.955))\r\n            camera.set_vertical_aperture(config.get('vertical_aperture', 15.29))\r\n\r\n            cameras.append({\r\n                'camera': camera,\r\n                'config': config,\r\n                'modalities': config.get('modalities', ['rgb'])\r\n            })\r\n\r\n        return cameras\r\n\r\n    def create_diverse_scenes(self, num_scenes=100):\r\n        \"\"\"Create multiple scene variations\"\"\"\r\n        scenes = []\r\n\r\n        for scene_idx in range(num_scenes):\r\n            # Randomize scene parameters\r\n            num_objects = random.randint(3, 10)\r\n            scene_config = {\r\n                'objects': [],\r\n                'lighting': self.randomize_lighting(),\r\n                'materials': self.randomize_materials(),\r\n                'layout': self.randomize_layout()\r\n            }\r\n\r\n            # Generate objects for this scene\r\n            for obj_idx in range(num_objects):\r\n                obj_config = {\r\n                    'type': random.choice(['cube', 'sphere', 'cylinder']),\r\n                    'position': [\r\n                        random.uniform(-3, 3),\r\n                        random.uniform(-3, 3),\r\n                        random.uniform(0.1, 2)\r\n                    ],\r\n                    'scale': [\r\n                        random.uniform(0.05, 0.3),\r\n                        random.uniform(0.05, 0.3),\r\n                        random.uniform(0.05, 0.3)\r\n                    ],\r\n                    'color': [\r\n                        random.uniform(0.1, 1.0),\r\n                        random.uniform(0.1, 1.0),\r\n                        random.uniform(0.1, 1.0)\r\n                    ]\r\n                }\r\n                scene_config['objects'].append(obj_config)\r\n\r\n            scenes.append(scene_config)\r\n\r\n        return scenes\r\n\r\n    def randomize_lighting(self):\r\n        \"\"\"Randomize lighting conditions\"\"\"\r\n        lighting_config = {\r\n            'intensity': random.uniform(1000, 5000),\r\n            'color': (\r\n                random.uniform(0.5, 1.0),\r\n                random.uniform(0.5, 1.0),\r\n                random.uniform(0.5, 1.0)\r\n            ),\r\n            'position': [\r\n                random.uniform(-5, 5),\r\n                random.uniform(-5, 5),\r\n                random.uniform(3, 8)\r\n            ],\r\n            'direction': [\r\n                random.uniform(-1, 1),\r\n                random.uniform(-1, 1),\r\n                random.uniform(-1, 0)\r\n            ]\r\n        }\r\n        return lighting_config\r\n\r\n    def randomize_materials(self):\r\n        \"\"\"Randomize material properties\"\"\"\r\n        material_config = {\r\n            'textures': [\r\n                'wood', 'metal', 'fabric', 'plastic', 'glass'\r\n            ],\r\n            'colors': [\r\n                (0.8, 0.2, 0.2),  # Red\r\n                (0.2, 0.8, 0.2),  # Green\r\n                (0.2, 0.2, 0.8),  # Blue\r\n                (0.8, 0.8, 0.2),  # Yellow\r\n                (0.8, 0.2, 0.8),  # Magenta\r\n            ],\r\n            'properties': {\r\n                'roughness': random.uniform(0.0, 1.0),\r\n                'metallic': random.uniform(0.0, 1.0),\r\n                'specular': random.uniform(0.0, 1.0)\r\n            }\r\n        }\r\n        return material_config\r\n\r\n    def randomize_layout(self):\r\n        \"\"\"Randomize object layout in scene\"\"\"\r\n        layout_config = {\r\n            'distribution': random.choice(['uniform', 'clustered', 'grid']),\r\n            'spacing': random.uniform(0.1, 0.5),\r\n            'orientation': random.uniform(0, 2 * np.pi)\r\n        }\r\n        return layout_config\r\n\r\ndef main():\r\n    # Initialize Isaac Sim world\r\n    world = World(stage_units_in_meters=1.0)\r\n\r\n    # Setup scene\r\n    scene_setup = SceneSetup(world)\r\n    scene_setup.setup_basic_environment()\r\n\r\n    # Create diverse scenes\r\n    scenes = scene_setup.create_diverse_scenes(num_scenes=50)\r\n\r\n    # Process each scene\r\n    for i, scene_config in enumerate(scenes):\r\n        print(f\"Setting up scene {i+1}/{len(scenes)}\")\r\n\r\n        # Clear previous objects\r\n        for obj in scene_setup.scene_objects:\r\n            # Remove object (implementation depends on Isaac Sim API)\r\n\r\n        # Add objects for current scene\r\n        scene_setup.add_objects_to_scene(scene_config['objects'])\r\n\r\n        # Apply lighting configuration\r\n        # Apply material configuration\r\n\r\n        # Wait for scene to stabilize\r\n        for _ in range(10):\r\n            world.step(render=True)\r\n\r\n    world.stop()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"domain-randomization-techniques",children:"Domain Randomization Techniques"}),"\n",(0,t.jsx)(e.h3,{id:"understanding-domain-randomization",children:"Understanding Domain Randomization"}),"\n",(0,t.jsx)(e.p,{children:"Domain randomization is a technique that increases the diversity of training data by varying environmental properties randomly during simulation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import random\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom typing import Dict, List, Tuple\r\n\r\n@dataclass\r\nclass DomainRandomizationConfig:\r\n    \"\"\"Configuration for domain randomization\"\"\"\r\n    lighting: Dict[str, Tuple[float, float]] = None\r\n    materials: Dict[str, Tuple[float, float]] = None\r\n    objects: Dict[str, Tuple[float, float]] = None\r\n    camera: Dict[str, Tuple[float, float]] = None\r\n    environment: Dict[str, Tuple[float, float]] = None\r\n\r\nclass DomainRandomizer:\r\n    def __init__(self, config: DomainRandomizationConfig):\r\n        self.config = config\r\n        self.step_count = 0\r\n\r\n    def randomize_lighting(self, stage):\r\n        \"\"\"Randomize lighting conditions in the scene\"\"\"\r\n        if not self.config.lighting:\r\n            return\r\n\r\n        # Randomize dome light\r\n        dome_light_path = \"/World/SkyDome\"\r\n        dome_light = get_prim_at_path(dome_light_path)\r\n        if dome_light:\r\n            intensity_range = self.config.lighting.get('intensity', (1000, 5000))\r\n            intensity = random.uniform(*intensity_range)\r\n            dome_light.GetAttribute(\"inputs:intensity\").Set(intensity)\r\n\r\n            color_range = self.config.lighting.get('color', ((0.5, 0.5, 0.5), (1.0, 1.0, 1.0)))\r\n            color = (\r\n                random.uniform(color_range[0][0], color_range[1][0]),\r\n                random.uniform(color_range[0][1], color_range[1][1]),\r\n                random.uniform(color_range[0][2], color_range[1][2])\r\n            )\r\n            dome_light.GetAttribute(\"inputs:color\").Set(color)\r\n\r\n        # Randomize directional lights\r\n        for i in range(3):  # Up to 3 directional lights\r\n            light_path = f\"/World/DirectionalLight_{i}\"\r\n            light = get_prim_at_path(light_path)\r\n            if light:\r\n                intensity = random.uniform(500, 2000)\r\n                light.GetAttribute(\"inputs:intensity\").Set(intensity)\r\n\r\n    def randomize_materials(self, stage):\r\n        \"\"\"Randomize material properties\"\"\"\r\n        if not self.config.materials:\r\n            return\r\n\r\n        materials_config = self.config.materials\r\n\r\n        # Iterate through all prims in the scene\r\n        for prim in stage.Traverse():\r\n            if prim.GetTypeName() in [\"Mesh\", \"Cube\", \"Sphere\", \"Cylinder\"]:\r\n                # Randomize diffuse color\r\n                if random.random() < materials_config.get('color_variation_prob', 0.5):\r\n                    color_range = materials_config.get('diffuse_color', ((0.1, 0.1, 0.1), (1.0, 1.0, 1.0)))\r\n                    new_color = (\r\n                        random.uniform(color_range[0][0], color_range[1][0]),\r\n                        random.uniform(color_range[0][1], color_range[1][1]),\r\n                        random.uniform(color_range[0][2], color_range[1][2])\r\n                    )\r\n\r\n                    # Apply to material (simplified)\r\n                    # In practice, you'd need to find and modify the material shader\r\n\r\n                # Randomize roughness\r\n                if random.random() < materials_config.get('roughness_variation_prob', 0.3):\r\n                    roughness_range = materials_config.get('roughness', (0.0, 1.0))\r\n                    new_roughness = random.uniform(*roughness_range)\r\n\r\n                # Randomize metallic\r\n                if random.random() < materials_config.get('metallic_variation_prob', 0.2):\r\n                    metallic_range = materials_config.get('metallic', (0.0, 1.0))\r\n                    new_metallic = random.uniform(*metallic_range)\r\n\r\n    def randomize_objects(self, stage):\r\n        \"\"\"Randomize object properties\"\"\"\r\n        if not self.config.objects:\r\n            return\r\n\r\n        objects_config = self.config.objects\r\n\r\n        # Randomize positions of objects\r\n        for prim in stage.Traverse():\r\n            if prim.GetTypeName() in [\"Mesh\", \"Cube\", \"Sphere\", \"Cylinder\"]:\r\n                if random.random() < objects_config.get('position_variation_prob', 0.7):\r\n                    pos_range = objects_config.get('position', {\r\n                        'x': (-2.0, 2.0),\r\n                        'y': (-2.0, 2.0),\r\n                        'z': (0.1, 2.0)\r\n                    })\r\n\r\n                    new_pos = [\r\n                        random.uniform(pos_range['x'][0], pos_range['x'][1]),\r\n                        random.uniform(pos_range['y'][0], pos_range['y'][1]),\r\n                        random.uniform(pos_range['z'][0], pos_range['z'][1])\r\n                    ]\r\n\r\n                    # Apply new position\r\n                    xform = UsdGeom.Xformable(prim)\r\n                    xform.ClearXformOpOrder()\r\n                    xform.AddTranslateOp().Set(new_pos)\r\n\r\n                # Randomize scale\r\n                if random.random() < objects_config.get('scale_variation_prob', 0.5):\r\n                    scale_range = objects_config.get('scale', (0.5, 2.0))\r\n                    new_scale = random.uniform(*scale_range)\r\n\r\n                    # Apply scale transformation\r\n                    scale_op = xform.AddScaleOp()\r\n                    scale_op.Set((new_scale, new_scale, new_scale))\r\n\r\n    def randomize_camera(self, cameras):\r\n        \"\"\"Randomize camera parameters\"\"\"\r\n        if not self.config.camera:\r\n            return\r\n\r\n        for camera_info in cameras:\r\n            camera = camera_info['camera']\r\n\r\n            # Randomize focal length\r\n            focal_range = self.config.camera.get('focal_length', (18.0, 55.0))\r\n            new_focal = random.uniform(*focal_range)\r\n            camera.set_focal_length(new_focal)\r\n\r\n            # Randomize aperture\r\n            aperture_range = self.config.camera.get('aperture', (1.4, 16.0))\r\n            new_aperture = random.uniform(*aperture_range)\r\n            camera.set_f_stop(new_aperture)\r\n\r\n            # Randomize camera position (with constraints)\r\n            pos_range = self.config.camera.get('position', {\r\n                'x': (-5.0, 5.0),\r\n                'y': (-5.0, 5.0),\r\n                'z': (1.0, 10.0)\r\n            })\r\n\r\n            new_pos = [\r\n                random.uniform(pos_range['x'][0], pos_range['x'][1]),\r\n                random.uniform(pos_range['y'][0], pos_range['y'][1]),\r\n                random.uniform(pos_range['z'][0], pos_range['z'][1])\r\n            ]\r\n\r\n            # Update camera position\r\n            camera.set_position(new_pos)\r\n\r\n    def apply_randomization(self, stage, cameras):\r\n        \"\"\"Apply all domain randomization effects\"\"\"\r\n        self.randomize_lighting(stage)\r\n        self.randomize_materials(stage)\r\n        self.randomize_objects(stage)\r\n        self.randomize_camera(cameras)\r\n\r\n        self.step_count += 1\r\n\r\n        # Reset randomization after certain steps\r\n        if self.step_count % 50 == 0:  # Reset every 50 steps\r\n            self.reset_randomization()\r\n\r\n    def reset_randomization(self):\r\n        \"\"\"Reset randomization for new episode\"\"\"\r\n        self.step_count = 0\r\n\r\ndef setup_domain_randomization():\r\n    \"\"\"Setup domain randomization configuration\"\"\"\r\n\r\n    config = DomainRandomizationConfig(\r\n        lighting={\r\n            'intensity': (1000, 5000),\r\n            'color': ((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))\r\n        },\r\n        materials={\r\n            'color_variation_prob': 0.8,\r\n            'roughness_variation_prob': 0.5,\r\n            'metallic_variation_prob': 0.3,\r\n            'diffuse_color': ((0.1, 0.1, 0.1), (1.0, 1.0, 1.0)),\r\n            'roughness': (0.0, 1.0),\r\n            'metallic': (0.0, 1.0)\r\n        },\r\n        objects={\r\n            'position_variation_prob': 0.7,\r\n            'scale_variation_prob': 0.5,\r\n            'position': {\r\n                'x': (-3.0, 3.0),\r\n                'y': (-3.0, 3.0),\r\n                'z': (0.1, 2.0)\r\n            },\r\n            'scale': (0.5, 2.0)\r\n        },\r\n        camera={\r\n            'focal_length': (18.0, 85.0),\r\n            'aperture': (1.4, 16.0),\r\n            'position': {\r\n                'x': (-5.0, 5.0),\r\n                'y': (-5.0, 5.0),\r\n                'z': (1.0, 10.0)\r\n            }\r\n        }\r\n    )\r\n\r\n    randomizer = DomainRandomizer(config)\r\n    return randomizer\n"})}),"\n",(0,t.jsx)(e.h2,{id:"multi-modal-data-generation",children:"Multi-Modal Data Generation"}),"\n",(0,t.jsx)(e.h3,{id:"rgb-data-capture",children:"RGB Data Capture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from PIL import Image\r\nimport numpy as np\r\nimport cv2\r\n\r\nclass RGBCapture:\r\n    def __init__(self, camera):\r\n        self.camera = camera\r\n\r\n    def capture_rgb_image(self):\r\n        \"\"\"Capture RGB image from camera\"\"\"\r\n        try:\r\n            # Get raw RGB data from camera\r\n            rgb_data = self.camera.get_rgb()\r\n\r\n            # Convert to PIL Image\r\n            if len(rgb_data.shape) == 3 and rgb_data.shape[2] == 3:\r\n                # Convert from RGB to BGR if needed for OpenCV\r\n                img_pil = Image.fromarray(rgb_data, mode=\"RGB\")\r\n                return img_pil\r\n            else:\r\n                raise ValueError(f\"Invalid RGB data shape: {rgb_data.shape}\")\r\n\r\n        except Exception as e:\r\n            print(f\"Error capturing RGB image: {e}\")\r\n            return None\r\n\r\n    def save_rgb_image(self, filepath, quality=95):\r\n        \"\"\"Save RGB image to file\"\"\"\r\n        img = self.capture_rgb_image()\r\n        if img:\r\n            img.save(filepath, \"PNG\", quality=quality)\r\n            return True\r\n        return False\r\n\r\n    def apply_augmentations(self, image, aug_params=None):\r\n        \"\"\"Apply data augmentations to RGB image\"\"\"\r\n        if aug_params is None:\r\n            aug_params = {\r\n                'brightness': 0.0,\r\n                'contrast': 1.0,\r\n                'saturation': 1.0,\r\n                'hue': 0.0,\r\n                'blur': 0.0,\r\n                'noise': 0.0\r\n            }\r\n\r\n        # Convert PIL to numpy for OpenCV operations\r\n        img_array = np.array(image)\r\n\r\n        # Apply brightness adjustment\r\n        if aug_params['brightness'] != 0:\r\n            img_array = np.clip(img_array.astype(np.float32) + aug_params['brightness'] * 255, 0, 255).astype(np.uint8)\r\n\r\n        # Apply contrast adjustment\r\n        if aug_params['contrast'] != 1.0:\r\n            img_array = np.clip((img_array.astype(np.float32) - 128) * aug_params['contrast'] + 128, 0, 255).astype(np.uint8)\r\n\r\n        # Apply saturation (convert to HSV, modify S channel)\r\n        if aug_params['saturation'] != 1.0:\r\n            hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\r\n            hsv[:, :, 1] = np.clip(hsv[:, :, 1].astype(np.float32) * aug_params['saturation'], 0, 255).astype(np.uint8)\r\n            img_array = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\r\n\r\n        # Apply blur\r\n        if aug_params['blur'] > 0:\r\n            kernel_size = int(aug_params['blur'] * 2) + 1\r\n            if kernel_size > 1:\r\n                img_array = cv2.GaussianBlur(img_array, (kernel_size, kernel_size), 0)\r\n\r\n        # Apply noise\r\n        if aug_params['noise'] > 0:\r\n            noise = np.random.normal(0, aug_params['noise'] * 255, img_array.shape).astype(np.uint8)\r\n            img_array = np.clip(img_array.astype(np.float32) + noise, 0, 255).astype(np.uint8)\r\n\r\n        # Convert back to PIL\r\n        augmented_img = Image.fromarray(img_array)\r\n        return augmented_img\n"})}),"\n",(0,t.jsx)(e.h3,{id:"depth-data-capture",children:"Depth Data Capture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\n\r\nclass DepthCapture:\r\n    def __init__(self, camera):\r\n        self.camera = camera\r\n\r\n    def capture_depth_data(self):\r\n        """Capture depth data from camera"""\r\n        try:\r\n            # Get raw depth data from camera\r\n            depth_data = self.camera.get_depth()\r\n\r\n            # Validate depth data\r\n            if depth_data is None:\r\n                raise ValueError("No depth data returned from camera")\r\n\r\n            # Convert to appropriate format (usually meters)\r\n            depth_array = np.array(depth_data)\r\n\r\n            # Handle invalid depth values (often represented as inf or -inf)\r\n            depth_array[np.isinf(depth_array)] = 0  # Set infinite values to 0\r\n            depth_array[depth_array < 0] = 0        # Set negative values to 0\r\n\r\n            return depth_array\r\n\r\n        except Exception as e:\r\n            print(f"Error capturing depth data: {e}")\r\n            return None\r\n\r\n    def save_depth_data(self, filepath):\r\n        """Save depth data to file"""\r\n        depth_data = self.capture_depth_data()\r\n        if depth_data is not None:\r\n            # Save as numpy array\r\n            np.save(filepath, depth_data)\r\n            return True\r\n        return False\r\n\r\n    def visualize_depth(self, depth_data, colormap=cv2.COLORMAP_JET):\r\n        """Visualize depth data with color mapping"""\r\n        if depth_data is None:\r\n            return None\r\n\r\n        # Normalize depth data for visualization\r\n        normalized_depth = (depth_data - np.min(depth_data)) / (np.max(depth_data) - np.min(depth_data))\r\n        normalized_depth = (normalized_depth * 255).astype(np.uint8)\r\n\r\n        # Apply color map\r\n        colored_depth = cv2.applyColorMap(normalized_depth, colormap)\r\n\r\n        # Convert to PIL Image\r\n        vis_img = Image.fromarray(colored_depth)\r\n        return vis_img\r\n\r\n    def generate_point_cloud(self, depth_data, camera_intrinsics):\r\n        """Generate point cloud from depth data"""\r\n        if depth_data is None:\r\n            return None\r\n\r\n        height, width = depth_data.shape\r\n        fx, fy = camera_intrinsics[\'fx\'], camera_intrinsics[\'fy\']\r\n        cx, cy = camera_intrinsics[\'cx\'], camera_intrinsics[\'cy\']\r\n\r\n        # Generate coordinate grids\r\n        x_coords, y_coords = np.meshgrid(np.arange(width), np.arange(height))\r\n\r\n        # Convert pixel coordinates to camera coordinates\r\n        x_cam = (x_coords - cx) * depth_data / fx\r\n        y_cam = (y_coords - cy) * depth_data / fy\r\n\r\n        # Stack to form point cloud\r\n        points = np.stack([x_cam, y_cam, depth_data], axis=-1)\r\n\r\n        # Reshape to (N, 3) format\r\n        points = points.reshape(-1, 3)\r\n\r\n        # Remove points with invalid depth\r\n        valid_points = points[~np.isnan(points).any(axis=1)]\r\n        valid_points = valid_points[~np.isinf(valid_points).any(axis=1)]\r\n\r\n        return valid_points\n'})}),"\n",(0,t.jsx)(e.h3,{id:"semantic-segmentation-capture",children:"Semantic Segmentation Capture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class SemanticSegmentationCapture:\r\n    def __init__(self, camera):\r\n        self.camera = camera\r\n        self.class_mapping = {}  # Maps semantic IDs to class names\r\n\r\n    def capture_segmentation_data(self):\r\n        """Capture semantic segmentation data"""\r\n        try:\r\n            # Get semantic segmentation from camera\r\n            seg_data = self.camera.get_semantic_segmentation()\r\n\r\n            if seg_data is None:\r\n                raise ValueError("No segmentation data returned from camera")\r\n\r\n            # Convert to numpy array\r\n            seg_array = np.array(seg_data)\r\n\r\n            return seg_array\r\n\r\n        except Exception as e:\r\n            print(f"Error capturing segmentation data: {e}")\r\n            return None\r\n\r\n    def save_segmentation_mask(self, filepath, format=\'png\'):\r\n        """Save segmentation mask to file"""\r\n        seg_data = self.capture_segmentation_data()\r\n        if seg_data is not None:\r\n            # Convert to PIL Image (assuming grayscale for class IDs)\r\n            seg_img = Image.fromarray(seg_data.astype(np.uint8), mode="L")\r\n            seg_img.save(filepath, format)\r\n            return True\r\n        return False\r\n\r\n    def create_colored_segmentation(self, seg_data, colormap=\'random\'):\r\n        """Create colored visualization of segmentation"""\r\n        if seg_data is None:\r\n            return None\r\n\r\n        # Create color mapping for each unique class\r\n        unique_classes = np.unique(seg_data)\r\n        color_map = {}\r\n\r\n        if colormap == \'random\':\r\n            for class_id in unique_classes:\r\n                if class_id != 0:  # Skip background\r\n                    color_map[class_id] = [\r\n                        random.randint(0, 255),\r\n                        random.randint(0, 255),\r\n                        random.randint(0, 255)\r\n                    ]\r\n                else:\r\n                    color_map[class_id] = [0, 0, 0]  # Black for background\r\n        else:\r\n            # Use predefined colormap\r\n            for i, class_id in enumerate(unique_classes):\r\n                color_map[class_id] = self.get_predefined_color(i)\r\n\r\n        # Create colored image\r\n        height, width = seg_data.shape\r\n        colored_seg = np.zeros((height, width, 3), dtype=np.uint8)\r\n\r\n        for class_id in unique_classes:\r\n            mask = (seg_data == class_id)\r\n            colored_seg[mask] = color_map[class_id]\r\n\r\n        # Convert to PIL Image\r\n        colored_img = Image.fromarray(colored_seg)\r\n        return colored_img\r\n\r\n    def get_predefined_color(self, index):\r\n        """Get predefined color from palette"""\r\n        colors = [\r\n            [0, 0, 0],        # Background\r\n            [128, 0, 0],      # Red\r\n            [0, 128, 0],      # Green\r\n            [128, 128, 0],    # Yellow\r\n            [0, 0, 128],      # Blue\r\n            [128, 0, 128],    # Purple\r\n            [0, 128, 128],    # Cyan\r\n            [128, 128, 128],  # Gray\r\n            [64, 0, 0],       # Dark red\r\n            [192, 0, 0],      # Bright red\r\n            # Add more colors as needed\r\n        ]\r\n        return colors[index % len(colors)]\r\n\r\n    def generate_instance_masks(self, seg_data):\r\n        """Generate separate masks for each instance"""\r\n        if seg_data is None:\r\n            return {}\r\n\r\n        instance_masks = {}\r\n        unique_instances = np.unique(seg_data)\r\n\r\n        for instance_id in unique_instances:\r\n            if instance_id != 0:  # Skip background\r\n                mask = (seg_data == instance_id).astype(np.uint8)\r\n                instance_masks[instance_id] = mask\r\n\r\n        return instance_masks\n'})}),"\n",(0,t.jsx)(e.h2,{id:"data-augmentation-techniques",children:"Data Augmentation Techniques"}),"\n",(0,t.jsx)(e.h3,{id:"geometric-transformations",children:"Geometric Transformations"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nclass GeometricAugmentation:\r\n    def __init__(self):\r\n        pass\r\n\r\n    def random_rotation(self, image, max_angle=15):\r\n        """Apply random rotation to image"""\r\n        angle = np.random.uniform(-max_angle, max_angle)\r\n        img_array = np.array(image)\r\n\r\n        # Get image dimensions\r\n        height, width = img_array.shape[:2]\r\n\r\n        # Calculate rotation matrix\r\n        center = (width // 2, height // 2)\r\n        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\r\n\r\n        # Apply rotation\r\n        rotated_img = cv2.warpAffine(img_array, rotation_matrix, (width, height),\r\n                                     borderMode=cv2.BORDER_REFLECT)\r\n\r\n        return Image.fromarray(rotated_img)\r\n\r\n    def random_translation(self, image, max_shift_ratio=0.1):\r\n        """Apply random translation to image"""\r\n        img_array = np.array(image)\r\n        height, width = img_array.shape[:2]\r\n\r\n        # Calculate max shift in pixels\r\n        max_shift_x = int(width * max_shift_ratio)\r\n        max_shift_y = int(height * max_shift_ratio)\r\n\r\n        # Random shifts\r\n        shift_x = np.random.randint(-max_shift_x, max_shift_x + 1)\r\n        shift_y = np.random.randint(-max_shift_y, max_shift_y + 1)\r\n\r\n        # Translation matrix\r\n        translation_matrix = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\r\n\r\n        # Apply translation\r\n        translated_img = cv2.warpAffine(img_array, translation_matrix, (width, height),\r\n                                        borderMode=cv2.BORDER_REFLECT)\r\n\r\n        return Image.fromarray(translated_img)\r\n\r\n    def random_scaling(self, image, scale_range=(0.8, 1.2)):\r\n        """Apply random scaling to image"""\r\n        img_array = np.array(image)\r\n        height, width = img_array.shape[:2]\r\n\r\n        # Random scale factor\r\n        scale_factor = np.random.uniform(scale_range[0], scale_range[1])\r\n\r\n        # Calculate new dimensions\r\n        new_width = int(width * scale_factor)\r\n        new_height = int(height * scale_factor)\r\n\r\n        # Resize image\r\n        scaled_img = cv2.resize(img_array, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\r\n\r\n        # Crop or pad to original size\r\n        if scale_factor > 1.0:  # Scale up, crop center\r\n            start_x = (new_width - width) // 2\r\n            start_y = (new_height - height) // 2\r\n            final_img = scaled_img[start_y:start_y + height, start_x:start_x + width]\r\n        else:  # Scale down, pad with reflection\r\n            pad_x = (width - new_width) // 2\r\n            pad_y = (height - new_height) // 2\r\n            final_img = np.pad(scaled_img,\r\n                              ((pad_y, height - new_height - pad_y),\r\n                               (pad_x, width - new_width - pad_x), (0, 0)),\r\n                              mode=\'reflect\')\r\n\r\n        return Image.fromarray(final_img)\r\n\r\n    def random_shear(self, image, max_shear=0.2):\r\n        """Apply random shearing to image"""\r\n        img_array = np.array(image)\r\n        height, width = img_array.shape[:2]\r\n\r\n        # Random shear factors\r\n        shear_x = np.random.uniform(-max_shear, max_shear)\r\n        shear_y = np.random.uniform(-max_shear, max_shear)\r\n\r\n        # Define three points before and after transformation\r\n        src_points = np.float32([[0, 0], [width, 0], [0, height]])\r\n        dst_points = np.float32([\r\n            [0, 0],\r\n            [width, shear_y * width],\r\n            [shear_x * height, height]\r\n        ])\r\n\r\n        # Calculate shear matrix\r\n        shear_matrix = cv2.getAffineTransform(src_points, dst_points)\r\n\r\n        # Apply shear\r\n        sheared_img = cv2.warpAffine(img_array, shear_matrix, (width, height),\r\n                                     borderMode=cv2.BORDER_REFLECT)\r\n\r\n        return Image.fromarray(sheared_img)\r\n\r\n    def random_flip(self, image, flip_prob=0.5):\r\n        """Apply random flipping to image"""\r\n        if np.random.rand() < flip_prob:\r\n            # Randomly choose between horizontal, vertical, or both\r\n            flip_type = np.random.choice([\'horizontal\', \'vertical\', \'both\'])\r\n\r\n            img_array = np.array(image)\r\n\r\n            if flip_type == \'horizontal\':\r\n                flipped_img = cv2.flip(img_array, 1)  # Horizontal flip\r\n            elif flip_type == \'vertical\':\r\n                flipped_img = cv2.flip(img_array, 0)  # Vertical flip\r\n            else:  # Both\r\n                flipped_img = cv2.flip(img_array, -1)  # Both flips\r\n\r\n            return Image.fromarray(flipped_img)\r\n\r\n        return image  # Return original if no flip\n'})}),"\n",(0,t.jsx)(e.h3,{id:"photometric-transformations",children:"Photometric Transformations"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class PhotometricAugmentation:\r\n    def __init__(self):\r\n        pass\r\n\r\n    def adjust_brightness(self, image, brightness_factor_range=(0.7, 1.3)):\r\n        """Adjust image brightness"""\r\n        factor = np.random.uniform(brightness_factor_range[0], brightness_factor_range[1])\r\n\r\n        img_array = np.array(image).astype(np.float32)\r\n        adjusted = img_array * factor\r\n\r\n        # Clip values to valid range\r\n        adjusted = np.clip(adjusted, 0, 255).astype(np.uint8)\r\n\r\n        return Image.fromarray(adjusted)\r\n\r\n    def adjust_contrast(self, image, contrast_factor_range=(0.8, 1.2)):\r\n        """Adjust image contrast"""\r\n        factor = np.random.uniform(contrast_factor_range[0], contrast_factor_range[1])\r\n\r\n        img_array = np.array(image).astype(np.float32)\r\n\r\n        # Subtract mean, multiply by factor, add mean back\r\n        mean = np.mean(img_array, axis=(0, 1), keepdims=True)\r\n        adjusted = (img_array - mean) * factor + mean\r\n\r\n        # Clip values to valid range\r\n        adjusted = np.clip(adjusted, 0, 255).astype(np.uint8)\r\n\r\n        return Image.fromarray(adjusted)\r\n\r\n    def adjust_saturation(self, image, saturation_factor_range=(0.8, 1.2)):\r\n        """Adjust image saturation"""\r\n        factor = np.random.uniform(saturation_factor_range[0], saturation_factor_range[1])\r\n\r\n        img_array = np.array(image).astype(np.float32)\r\n\r\n        # Convert to grayscale (take luminance)\r\n        gray = np.dot(img_array[...,:3], [0.2989, 0.5870, 0.1140])\r\n        gray = gray[..., np.newaxis]  # Make it broadcastable\r\n\r\n        # Blend original and grayscale\r\n        saturated = img_array * factor + gray * (1 - factor)\r\n\r\n        # Clip values to valid range\r\n        saturated = np.clip(saturated, 0, 255).astype(np.uint8)\r\n\r\n        return Image.fromarray(saturated)\r\n\r\n    def adjust_hue(self, image, hue_delta=0.1):\r\n        """Adjust image hue"""\r\n        delta = np.random.uniform(-hue_delta, hue_delta)\r\n\r\n        img_array = np.array(image).astype(np.float32) / 255.0\r\n\r\n        # Convert RGB to HSV\r\n        hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\r\n\r\n        # Adjust hue\r\n        hsv[:, :, 0] = (hsv[:, :, 0] + delta) % 1.0\r\n\r\n        # Convert back to RGB\r\n        rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\r\n\r\n        # Convert back to uint8\r\n        adjusted = (rgb * 255).astype(np.uint8)\r\n\r\n        return Image.fromarray(adjusted)\r\n\r\n    def add_noise(self, image, noise_std_range=(0.0, 0.05)):\r\n        """Add random noise to image"""\r\n        std = np.random.uniform(noise_std_range[0], noise_std_range[1])\r\n\r\n        img_array = np.array(image).astype(np.float32)\r\n\r\n        # Generate random noise\r\n        noise = np.random.normal(0, std * 255, img_array.shape).astype(np.float32)\r\n\r\n        # Add noise\r\n        noisy_img = img_array + noise\r\n\r\n        # Clip values to valid range\r\n        noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\r\n\r\n        return Image.fromarray(noisy_img)\r\n\r\n    def adjust_gamma(self, image, gamma_range=(0.8, 1.2)):\r\n        """Adjust image gamma"""\r\n        gamma = np.random.uniform(gamma_range[0], gamma_range[1])\r\n\r\n        img_array = np.array(image).astype(np.float32)\r\n\r\n        # Apply gamma correction\r\n        corrected = 255.0 * np.power(img_array / 255.0, 1.0 / gamma)\r\n\r\n        # Clip values to valid range\r\n        corrected = np.clip(corrected, 0, 255).astype(np.uint8)\r\n\r\n        return Image.fromarray(corrected)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"data-quality-validation",children:"Data Quality Validation"}),"\n",(0,t.jsx)(e.h3,{id:"synthetic-data-quality-metrics",children:"Synthetic Data Quality Metrics"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import numpy as np\r\nfrom scipy import ndimage\r\nfrom skimage.metrics import structural_similarity as ssim\r\nfrom skimage.feature import canny\r\nimport cv2\r\n\r\nclass SyntheticDataValidator:\r\n    def __init__(self):\r\n        pass\r\n\r\n    def assess_realism_score(self, synthetic_img, real_img):\r\n        \"\"\"Assess how realistic synthetic data appears compared to real data\"\"\"\r\n        syn_array = np.array(synthetic_img)\r\n        real_array = np.array(real_img)\r\n\r\n        # Structural Similarity Index (SSIM)\r\n        ssim_score = ssim(syn_array, real_array, multichannel=True, data_range=255)\r\n\r\n        # Edge density comparison\r\n        syn_edges = canny(cv2.cvtColor(syn_array, cv2.COLOR_RGB2GRAY))\r\n        real_edges = canny(cv2.cvtColor(real_array, cv2.COLOR_RGB2GRAY))\r\n\r\n        syn_edge_density = np.sum(syn_edges) / syn_edges.size\r\n        real_edge_density = np.sum(real_edges) / real_edges.size\r\n\r\n        edge_similarity = 1 - abs(syn_edge_density - real_edge_density) / max(syn_edge_density, real_edge_density, 1e-6)\r\n\r\n        # Color distribution similarity\r\n        syn_hist = [cv2.calcHist([syn_array], [i], None, [256], [0, 256]) for i in range(3)]\r\n        real_hist = [cv2.calcHist([real_array], [i], None, [256], [0, 256]) for i in range(3)]\r\n\r\n        hist_similarity = 0\r\n        for i in range(3):\r\n            correlation = cv2.compareHist(syn_hist[i], real_hist[i], cv2.HISTCMP_CORREL)\r\n            hist_similarity += correlation\r\n\r\n        hist_similarity /= 3\r\n\r\n        # Combine scores (weights can be adjusted)\r\n        realism_score = 0.4 * ssim_score + 0.3 * edge_similarity + 0.3 * hist_similarity\r\n\r\n        return {\r\n            'ssim': ssim_score,\r\n            'edge_similarity': edge_similarity,\r\n            'histogram_similarity': hist_similarity,\r\n            'realism_score': realism_score\r\n        }\r\n\r\n    def validate_annotation_quality(self, annotations, image_shape):\r\n        \"\"\"Validate quality of synthetic annotations\"\"\"\r\n        validation_results = {\r\n            'valid_objects': 0,\r\n            'invalid_objects': 0,\r\n            'occlusion_issues': 0,\r\n            'annotation_completeness': 0.0\r\n        }\r\n\r\n        total_pixels = image_shape[0] * image_shape[1]\r\n        annotated_pixels = 0\r\n\r\n        for obj in annotations.get('objects', []):\r\n            bbox = obj.get('bbox', {})\r\n            if bbox:\r\n                min_pt = bbox.get('min', [])\r\n                max_pt = bbox.get('max', [])\r\n\r\n                if len(min_pt) >= 2 and len(max_pt) >= 2:\r\n                    width = max_pt[0] - min_pt[0]\r\n                    height = max_pt[1] - min_pt[1]\r\n\r\n                    if width > 0 and height > 0:  # Valid bounding box\r\n                        validation_results['valid_objects'] += 1\r\n                        obj_area = width * height\r\n                        annotated_pixels += obj_area\r\n\r\n                        # Check for potential occlusion issues\r\n                        if width * height > total_pixels * 0.8:  # Object too large\r\n                            validation_results['occlusion_issues'] += 1\r\n                    else:\r\n                        validation_results['invalid_objects'] += 1\r\n                else:\r\n                    validation_results['invalid_objects'] += 1\r\n\r\n        validation_results['annotation_completeness'] = annotated_pixels / total_pixels if total_pixels > 0 else 0\r\n\r\n        return validation_results\r\n\r\n    def check_domain_gap(self, synthetic_data, real_data_stats):\r\n        \"\"\"Check the domain gap between synthetic and real data\"\"\"\r\n        syn_array = np.array(synthetic_data)\r\n\r\n        # Calculate synthetic data statistics\r\n        syn_mean = np.mean(syn_array, axis=(0, 1))\r\n        syn_std = np.std(syn_array, axis=(0, 1))\r\n        syn_median = np.median(syn_array, axis=(0, 1))\r\n\r\n        # Compare with real data statistics\r\n        mean_diff = np.abs(syn_mean - real_data_stats['mean'])\r\n        std_diff = np.abs(syn_std - real_data_stats['std'])\r\n\r\n        # Calculate domain gap score (lower is better)\r\n        gap_score = np.mean(mean_diff) + np.mean(std_diff)\r\n\r\n        return {\r\n            'mean_difference': mean_diff.tolist(),\r\n            'std_difference': std_diff.tolist(),\r\n            'median_difference': np.abs(syn_median - real_data_stats['median']).tolist(),\r\n            'domain_gap_score': float(gap_score)\r\n        }\r\n\r\n    def validate_consistency_across_modalities(self, rgb_data, depth_data, seg_data):\r\n        \"\"\"Validate consistency between different modalities\"\"\"\r\n        consistency_report = {\r\n            'rgb_depth_alignment': True,\r\n            'depth_seg_alignment': True,\r\n            'occlusion_consistency': True,\r\n            'overall_consistency_score': 0.0\r\n        }\r\n\r\n        if rgb_data is None or depth_data is None or seg_data is None:\r\n            consistency_report['overall_consistency_score'] = 0.0\r\n            return consistency_report\r\n\r\n        # Check that all modalities have the same dimensions\r\n        rgb_shape = rgb_data.shape[:2]\r\n        depth_shape = depth_data.shape\r\n        seg_shape = seg_data.shape\r\n\r\n        if not (rgb_shape == depth_shape == seg_shape):\r\n            consistency_report['rgb_depth_alignment'] = False\r\n            consistency_report['overall_consistency_score'] = 0.0\r\n            return consistency_report\r\n\r\n        # Check depth-segmentation consistency\r\n        # Objects in segmentation should have consistent depth values\r\n        unique_classes = np.unique(seg_data)\r\n        depth_consistency = 0\r\n        total_classes = 0\r\n\r\n        for class_id in unique_classes:\r\n            if class_id != 0:  # Skip background\r\n                mask = (seg_data == class_id)\r\n                class_depth_values = depth_data[mask]\r\n\r\n                if len(class_depth_values) > 10:  # Need sufficient pixels for statistics\r\n                    class_depth_std = np.std(class_depth_values)\r\n                    class_depth_mean = np.mean(class_depth_values)\r\n\r\n                    # Depth should be relatively consistent for the same object\r\n                    if class_depth_std < class_depth_mean * 0.1:  # Threshold can be adjusted\r\n                        depth_consistency += 1\r\n                    total_classes += 1\r\n\r\n        if total_classes > 0:\r\n            consistency_report['depth_seg_alignment'] = (depth_consistency / total_classes) > 0.7\r\n        else:\r\n            consistency_report['depth_seg_alignment'] = True\r\n\r\n        # Calculate overall consistency score\r\n        alignment_score = 1.0 if consistency_report['rgb_depth_alignment'] else 0.0\r\n        seg_depth_score = 1.0 if consistency_report['depth_seg_alignment'] else 0.0\r\n\r\n        consistency_report['overall_consistency_score'] = (alignment_score + seg_depth_score) / 2.0\r\n\r\n        return consistency_report\n"})}),"\n",(0,t.jsx)(e.h2,{id:"dataset-organization-and-management",children:"Dataset Organization and Management"}),"\n",(0,t.jsx)(e.h3,{id:"dataset-structure",children:"Dataset Structure"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import os\r\nimport json\r\nimport shutil\r\nfrom pathlib import Path\r\n\r\nclass SyntheticDatasetManager:\r\n    def __init__(self, dataset_path):\r\n        self.dataset_path = Path(dataset_path)\r\n        self.annotations_path = self.dataset_path / \"annotations\"\r\n        self.images_path = self.dataset_path / \"images\"\r\n        self.depth_path = self.dataset_path / \"depth\"\r\n        self.seg_path = self.dataset_path / \"segmentation\"\r\n\r\n        # Create directory structure\r\n        self._create_directories()\r\n\r\n    def _create_directories(self):\r\n        \"\"\"Create dataset directory structure\"\"\"\r\n        dirs_to_create = [\r\n            self.dataset_path,\r\n            self.annotations_path,\r\n            self.images_path,\r\n            self.depth_path,\r\n            self.seg_path\r\n        ]\r\n\r\n        for dir_path in dirs_to_create:\r\n            dir_path.mkdir(parents=True, exist_ok=True)\r\n\r\n    def save_sample(self, sample_id, rgb_image, depth_data, seg_data, annotations):\r\n        \"\"\"Save a complete data sample\"\"\"\r\n        # Save RGB image\r\n        rgb_path = self.images_path / f\"{sample_id}.png\"\r\n        rgb_image.save(str(rgb_path))\r\n\r\n        # Save depth data\r\n        depth_path = self.depth_path / f\"{sample_id}.npy\"\r\n        np.save(str(depth_path), depth_data)\r\n\r\n        # Save segmentation mask\r\n        seg_path = self.seg_path / f\"{sample_id}.png\"\r\n        seg_img = Image.fromarray(seg_data.astype(np.uint8), mode=\"L\")\r\n        seg_img.save(str(seg_path))\r\n\r\n        # Save annotations\r\n        annotation_path = self.annotations_path / f\"{sample_id}.json\"\r\n        with open(annotation_path, 'w') as f:\r\n            json.dump(annotations, f, indent=2)\r\n\r\n    def create_train_val_split(self, split_ratio=0.8):\r\n        \"\"\"Create train/validation split\"\"\"\r\n        all_samples = [f.stem for f in self.annotations_path.glob(\"*.json\")]\r\n\r\n        # Shuffle samples\r\n        np.random.shuffle(all_samples)\r\n\r\n        split_idx = int(len(all_samples) * split_ratio)\r\n        train_samples = all_samples[:split_idx]\r\n        val_samples = all_samples[split_idx:]\r\n\r\n        # Create splits directory\r\n        splits_path = self.dataset_path / \"splits\"\r\n        splits_path.mkdir(exist_ok=True)\r\n\r\n        # Save split information\r\n        split_info = {\r\n            'train': train_samples,\r\n            'val': val_samples,\r\n            'split_ratio': split_ratio\r\n        }\r\n\r\n        with open(splits_path / \"split_info.json\", 'w') as f:\r\n            json.dump(split_info, f, indent=2)\r\n\r\n        return split_info\r\n\r\n    def generate_dataset_metadata(self):\r\n        \"\"\"Generate comprehensive dataset metadata\"\"\"\r\n        samples = list(self.annotations_path.glob(\"*.json\"))\r\n\r\n        metadata = {\r\n            'dataset_name': self.dataset_path.name,\r\n            'total_samples': len(samples),\r\n            'modalities': ['rgb', 'depth', 'segmentation'],\r\n            'created_at': str(np.datetime64('now')),\r\n            'generator': 'Isaac Sim Synthetic Data Generator',\r\n            'version': '1.0',\r\n            'license': 'MIT',\r\n            'statistics': {\r\n                'rgb_formats': [],\r\n                'depth_ranges': {},\r\n                'class_distribution': {}\r\n            }\r\n        }\r\n\r\n        # Calculate statistics\r\n        if samples:\r\n            first_sample = samples[0]\r\n            with open(first_sample, 'r') as f:\r\n                first_annotation = json.load(f)\r\n\r\n            # Class distribution\r\n            if 'objects' in first_annotation:\r\n                classes = [obj.get('class', 'unknown') for obj in first_annotation['objects']]\r\n                from collections import Counter\r\n                class_counts = Counter(classes)\r\n                metadata['statistics']['class_distribution'] = dict(class_counts)\r\n\r\n        # Save metadata\r\n        metadata_path = self.dataset_path / \"metadata.json\"\r\n        with open(metadata_path, 'w') as f:\r\n            json.dump(metadata, f, indent=2)\r\n\r\n        return metadata\r\n\r\n    def validate_dataset_integrity(self):\r\n        \"\"\"Validate that all samples have complete data\"\"\"\r\n        samples = [f.stem for f in self.annotations_path.glob(\"*.json\")]\r\n        integrity_issues = []\r\n\r\n        for sample_id in samples:\r\n            missing_modalities = []\r\n\r\n            # Check RGB image\r\n            if not (self.images_path / f\"{sample_id}.png\").exists():\r\n                missing_modalities.append('rgb')\r\n\r\n            # Check depth data\r\n            if not (self.depth_path / f\"{sample_id}.npy\").exists():\r\n                missing_modalities.append('depth')\r\n\r\n            # Check segmentation\r\n            if not (self.seg_path / f\"{sample_id}.png\").exists():\r\n                missing_modalities.append('segmentation')\r\n\r\n            # Check annotation\r\n            if not (self.annotations_path / f\"{sample_id}.json\").exists():\r\n                missing_modalities.append('annotation')\r\n\r\n            if missing_modalities:\r\n                integrity_issues.append({\r\n                    'sample_id': sample_id,\r\n                    'missing_modalities': missing_modalities\r\n                })\r\n\r\n        return {\r\n            'total_samples': len(samples),\r\n            'complete_samples': len(samples) - len(integrity_issues),\r\n            'integrity_issues': integrity_issues,\r\n            'integrity_score': (len(samples) - len(integrity_issues)) / len(samples) if samples else 0\r\n        }\r\n\r\ndef main():\r\n    # Example usage of the synthetic data generation pipeline\r\n    print(\"Setting up synthetic data generation pipeline...\")\r\n\r\n    # Initialize dataset manager\r\n    dataset_manager = SyntheticDatasetManager(\"synthetic_robotics_dataset\")\r\n\r\n    # Create sample data (in practice, this would come from Isaac Sim)\r\n    for i in range(10):  # Generate 10 sample images\r\n        # Create dummy data for example\r\n        rgb_img = Image.new('RGB', (640, 480), color=(255, 255, 255))\r\n        depth_data = np.random.rand(480, 640).astype(np.float32) * 10.0  # 0-10m depth\r\n        seg_data = np.random.randint(0, 5, size=(480, 640)).astype(np.int32)  # 5 semantic classes\r\n\r\n        annotations = {\r\n            'sample_id': f'sample_{i}',\r\n            'timestamp': f'timestamp_{i}',\r\n            'objects': [\r\n                {\r\n                    'class': 'object_1',\r\n                    'bbox': {\r\n                        'min': [50, 50],\r\n                        'max': [100, 100]\r\n                    },\r\n                    'pose': {\r\n                        'position': [1, 2, 3],\r\n                        'orientation': [0, 0, 0, 1]\r\n                    }\r\n                }\r\n            ]\r\n        }\r\n\r\n        dataset_manager.save_sample(f'sample_{i}', rgb_img, depth_data, seg_data, annotations)\r\n\r\n    # Create train/val split\r\n    split_info = dataset_manager.create_train_val_split(split_ratio=0.8)\r\n    print(f\"Created train/val split: {len(split_info['train'])} train, {len(split_info['val'])} val\")\r\n\r\n    # Generate metadata\r\n    metadata = dataset_manager.generate_dataset_metadata()\r\n    print(f\"Generated dataset metadata for {metadata['total_samples']} samples\")\r\n\r\n    # Validate dataset integrity\r\n    integrity_report = dataset_manager.validate_dataset_integrity()\r\n    print(f\"Dataset integrity: {integrity_report['integrity_score']:.2f}\")\r\n\r\n    print(\"Synthetic data generation pipeline setup complete!\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives-review",children:"Learning Objectives Review"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the importance and benefits of synthetic data in robotics \u2713"}),"\n",(0,t.jsx)(e.li,{children:"Learn techniques for generating diverse synthetic datasets \u2713"}),"\n",(0,t.jsx)(e.li,{children:"Implement domain randomization strategies for robust training \u2713"}),"\n",(0,t.jsx)(e.li,{children:"Create multi-modal synthetic data (RGB, depth, segmentation) \u2713"}),"\n",(0,t.jsx)(e.li,{children:"Apply data augmentation techniques for synthetic datasets \u2713"}),"\n",(0,t.jsx)(e.li,{children:"Validate synthetic data quality and effectiveness \u2713"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Set up Isaac Sim with domain randomization"}),"\n",(0,t.jsx)(e.li,{children:"Create a synthetic dataset with RGB, depth, and segmentation modalities"}),"\n",(0,t.jsx)(e.li,{children:"Apply domain randomization techniques to increase dataset diversity"}),"\n",(0,t.jsx)(e.li,{children:"Implement data augmentation pipelines"}),"\n",(0,t.jsx)(e.li,{children:"Validate the quality of generated synthetic data"}),"\n",(0,t.jsx)(e.li,{children:"Organize the dataset according to standard formats"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Explain the concept of domain randomization and its importance in synthetic data generation."}),"\n",(0,t.jsx)(e.li,{children:"What are the key components of a synthetic data generation pipeline?"}),"\n",(0,t.jsx)(e.li,{children:"How do you validate the quality of synthetic datasets?"}),"\n",(0,t.jsx)(e.li,{children:"What are the advantages of multi-modal synthetic data over single modality?"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:'"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network" - Ledig et al.'}),"\n",(0,t.jsx)(e.li,{children:'"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World" - Tobin et al.'}),"\n",(0,t.jsx)(e.li,{children:'"Synthetic Data Generation for End-to-End Thermal Infrared Pedestrian Detection" - H\xfcbner et al.'}),"\n",(0,t.jsxs)(e.li,{children:["NVIDIA Isaac Sim Documentation: ",(0,t.jsx)(e.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/",children:"https://docs.omniverse.nvidia.com/isaacsim/latest/"})]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(e.p,{children:["Continue to ",(0,t.jsx)(e.a,{href:"/physical-ai/docs/module-3/isaac-vslam",children:"Isaac ROS VSLAM"})," to learn about visual SLAM implementation using Isaac ROS."]})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453(n,e,r){r.d(e,{R:()=>s,x:()=>o});var a=r(6540);const t={},i=a.createContext(t);function s(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),a.createElement(i.Provider,{value:e},n.children)}}}]);