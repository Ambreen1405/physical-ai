"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[648],{6250(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-3/intro","title":"Module 3 Overview - The AI-Robot Brain","description":"Introduction to NVIDIA Isaac platform for AI-powered robotics","source":"@site/docs/module-3/intro.md","sourceDirName":"module-3","slug":"/module-3/intro","permalink":"/physical-ai/docs/module-3/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-textbook/book-ai/tree/main/docs/module-3/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"intro","title":"Module 3 Overview - The AI-Robot Brain","sidebar_position":1,"description":"Introduction to NVIDIA Isaac platform for AI-powered robotics","keywords":["nvidia isaac","ai robotics","robotics platform","deep learning","computer vision","robotics ai"]},"sidebar":"textbook","previous":{"title":"Module 2 Assessment","permalink":"/physical-ai/docs/module-2/assessment"},"next":{"title":"NVIDIA Isaac Introduction","permalink":"/physical-ai/docs/module-3/nvidia-isaac-intro"}}');var s=i(4848),a=i(8453);const t={id:"intro",title:"Module 3 Overview - The AI-Robot Brain",sidebar_position:1,description:"Introduction to NVIDIA Isaac platform for AI-powered robotics",keywords:["nvidia isaac","ai robotics","robotics platform","deep learning","computer vision","robotics ai"]},o="Module 3: The AI-Robot Brain (NVIDIA Isaac)",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"The AI-Robot Brain Concept",id:"the-ai-robot-brain-concept",level:2},{value:"NVIDIA Isaac Ecosystem",id:"nvidia-isaac-ecosystem",level:2},{value:"Isaac Sim",id:"isaac-sim",level:3},{value:"Isaac ROS",id:"isaac-ros",level:3},{value:"Isaac Apps",id:"isaac-apps",level:3},{value:"Isaac Lab",id:"isaac-lab",level:3},{value:"AI in Robotics",id:"ai-in-robotics",level:2},{value:"Computer Vision for Robotics",id:"computer-vision-for-robotics",level:3},{value:"Deep Learning Integration",id:"deep-learning-integration",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Code Example: Isaac ROS Visual SLAM",id:"code-example-isaac-ros-visual-slam",level:2},{value:"Learning Objectives Review",id:"learning-objectives-review",level:2},{value:"Practical Exercise",id:"practical-exercise",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-3-the-ai-robot-brain-nvidia-isaac",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac)"})}),"\n",(0,s.jsx)(n.p,{children:"Welcome to Module 3 of the Physical AI & Humanoid Robotics textbook! This module focuses on the NVIDIA Isaac platform, which brings AI and deep learning capabilities to robotics applications. You'll learn how to integrate advanced AI techniques into robotic systems for perception, planning, and control."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the NVIDIA Isaac platform architecture and components"}),"\n",(0,s.jsx)(n.li,{children:"Set up and configure Isaac Sim for photorealistic simulation"}),"\n",(0,s.jsx)(n.li,{children:"Generate synthetic training data for robotics applications"}),"\n",(0,s.jsx)(n.li,{children:"Implement visual SLAM using Isaac ROS VSLAM"}),"\n",(0,s.jsx)(n.li,{children:"Apply Nav2 for path planning in humanoid robots"}),"\n",(0,s.jsx)(n.li,{children:"Deploy AI models from simulation to real robots using sim-to-real transfer"}),"\n",(0,s.jsx)(n.li,{children:"Build perception pipelines for autonomous robotic systems"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,s.jsx)(n.p,{children:"This module consists of the following chapters:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/physical-ai/docs/module-3/nvidia-isaac-intro",children:"NVIDIA Isaac Introduction"})," - Platform overview and setup"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/physical-ai/docs/module-3/isaac-sim",children:"Isaac Sim"})," - Photorealistic simulation environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/physical-ai/docs/module-3/synthetic-data",children:"Synthetic Data Generation"})," - Training dataset creation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/physical-ai/docs/module-3/isaac-vslam",children:"Isaac ROS VSLAM"})," - Visual SLAM implementation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/physical-ai/docs/module-3/nav2-path-planning",children:"Nav2 Path Planning"})," - Bipedal navigation systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/physical-ai/docs/module-3/sim-to-real",children:"Sim-to-Real Transfer"})," - Deployment techniques"]}),"\n",(0,s.jsx)(n.li,{children:"Comprehensive evaluation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-ai-robot-brain-concept",children:"The AI-Robot Brain Concept"}),"\n",(0,s.jsx)(n.p,{children:'The "AI-Robot Brain" represents the integration of artificial intelligence with robotics systems, enabling robots to perceive, reason, and act intelligently in complex environments. The NVIDIA Isaac platform provides the tools and frameworks necessary to implement these capabilities:'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),": Computer vision, sensor fusion, object detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reasoning"}),": Path planning, decision making, task planning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Motion control, manipulation, locomotion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning"}),": Deep learning, reinforcement learning, imitation learning"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"nvidia-isaac-ecosystem",children:"NVIDIA Isaac Ecosystem"}),"\n",(0,s.jsx)(n.p,{children:"The NVIDIA Isaac ecosystem includes several key components:"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-sim",children:"Isaac Sim"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"High-fidelity photorealistic simulation environment"}),"\n",(0,s.jsx)(n.li,{children:"Domain randomization capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Synthetic data generation tools"}),"\n",(0,s.jsx)(n.li,{children:"Physics-accurate simulation"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros",children:"Isaac ROS"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU-accelerated perception and navigation"}),"\n",(0,s.jsx)(n.li,{children:"Visual SLAM and mapping"}),"\n",(0,s.jsx)(n.li,{children:"Sensor processing pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Hardware acceleration"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-apps",children:"Isaac Apps"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Pre-built applications for common robotics tasks"}),"\n",(0,s.jsx)(n.li,{children:"Reference implementations"}),"\n",(0,s.jsx)(n.li,{children:"Best practices and examples"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-lab",children:"Isaac Lab"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reinforcement learning environment"}),"\n",(0,s.jsx)(n.li,{children:"Physics simulation with GPU acceleration"}),"\n",(0,s.jsx)(n.li,{children:"Training and deployment tools"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ai-in-robotics",children:"AI in Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"computer-vision-for-robotics",children:"Computer Vision for Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Modern robotics heavily relies on computer vision for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Object detection and recognition"}),"\n",(0,s.jsx)(n.li,{children:"Semantic segmentation"}),"\n",(0,s.jsx)(n.li,{children:"Depth estimation"}),"\n",(0,s.jsx)(n.li,{children:"Visual odometry"}),"\n",(0,s.jsx)(n.li,{children:"Scene understanding"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"deep-learning-integration",children:"Deep Learning Integration"}),"\n",(0,s.jsx)(n.p,{children:"The Isaac platform enables:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU-accelerated inference"}),"\n",(0,s.jsx)(n.li,{children:"Model training and optimization"}),"\n",(0,s.jsx)(n.li,{children:"Edge deployment"}),"\n",(0,s.jsx)(n.li,{children:"Real-time processing"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,s.jsx)(n.p,{children:"Critical for robotics applications:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Domain randomization"}),"\n",(0,s.jsx)(n.li,{children:"Synthetic data training"}),"\n",(0,s.jsx)(n.li,{children:"Reality gap bridging"}),"\n",(0,s.jsx)(n.li,{children:"Transfer learning techniques"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting this module, you should have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed Module 1 (ROS 2 fundamentals) and Module 2 (simulation)"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of basic machine learning concepts"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with Python and deep learning frameworks"}),"\n",(0,s.jsx)(n.li,{children:"Experience with computer vision concepts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,s.jsx)(n.p,{children:"Each chapter in this module includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Theoretical concepts with practical examples"}),"\n",(0,s.jsx)(n.li,{children:"Step-by-step tutorials for Isaac platform setup"}),"\n",(0,s.jsx)(n.li,{children:"Exercises to reinforce learning"}),"\n",(0,s.jsx)(n.li,{children:"Assessment questions"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"code-example-isaac-ros-visual-slam",children:"Code Example: Isaac ROS Visual SLAM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass IsaacVSLAMNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_vslam_node\')\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to camera images\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_rect_color\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Subscribe to camera info\r\n        self.info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/camera_info\',\r\n            self.info_callback,\r\n            10\r\n        )\r\n\r\n        # Publish pose estimates\r\n        self.pose_pub = self.create_publisher(\r\n            PoseStamped,\r\n            \'/visual_slam/pose\',\r\n            10\r\n        )\r\n\r\n        # Initialize VSLAM components\r\n        self.orb_detector = cv2.ORB_create(nfeatures=2000)\r\n        self.bf_matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\r\n\r\n        self.prev_keypoints = None\r\n        self.prev_descriptors = None\r\n        self.current_pose = np.eye(4)  # 4x4 transformation matrix\r\n\r\n        self.camera_matrix = None\r\n        self.distortion_coeffs = None\r\n\r\n        self.get_logger().info(\'Isaac VSLAM node initialized\')\r\n\r\n    def info_callback(self, msg):\r\n        """Receive camera calibration information"""\r\n        if self.camera_matrix is None:\r\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n            self.distortion_coeffs = np.array(msg.d)\r\n            self.get_logger().info(\'Camera calibration received\')\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera images for VSLAM"""\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n\r\n            # Detect and compute features\r\n            keypoints = self.orb_detector.detect(cv_image)\r\n            keypoints, descriptors = self.orb_detector.compute(cv_image, keypoints)\r\n\r\n            if self.prev_keypoints is not None and len(keypoints) > 10:\r\n                # Match features between frames\r\n                matches = self.bf_matcher.knnMatch(\r\n                    self.prev_descriptors, descriptors, k=2\r\n                )\r\n\r\n                # Apply Lowe\'s ratio test\r\n                good_matches = []\r\n                for match_pair in matches:\r\n                    if len(match_pair) == 2:\r\n                        m, n = match_pair\r\n                        if m.distance < 0.75 * n.distance:\r\n                            good_matches.append(m)\r\n\r\n                if len(good_matches) >= 10:\r\n                    # Extract matched points\r\n                    prev_pts = np.float32([\r\n                        self.prev_keypoints[m.queryIdx].pt\r\n                        for m in good_matches\r\n                    ]).reshape(-1, 1, 2)\r\n\r\n                    curr_pts = np.float32([\r\n                        keypoints[m.trainIdx].pt\r\n                        for m in good_matches\r\n                    ]).reshape(-1, 1, 2)\r\n\r\n                    # Estimate motion using Essential Matrix\r\n                    if self.camera_matrix is not None:\r\n                        E, mask = cv2.findEssentialMat(\r\n                            curr_pts, prev_pts,\r\n                            self.camera_matrix,\r\n                            threshold=1, prob=0.999\r\n                        )\r\n\r\n                        if E is not None:\r\n                            # Recover pose\r\n                            _, R, t, mask = cv2.recoverPose(\r\n                                E, curr_pts, prev_pts,\r\n                                self.camera_matrix\r\n                            )\r\n\r\n                            # Update current pose\r\n                            delta_transform = np.eye(4)\r\n                            delta_transform[:3, :3] = R\r\n                            delta_transform[:3, 3] = t.flatten()\r\n\r\n                            self.current_pose = self.current_pose @ np.linalg.inv(delta_transform)\r\n\r\n                            # Publish pose estimate\r\n                            self.publish_pose_estimate(msg.header)\r\n\r\n            # Store current frame for next iteration\r\n            self.prev_keypoints = keypoints\r\n            self.prev_descriptors = descriptors\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error in VSLAM processing: {e}\')\r\n\r\n    def publish_pose_estimate(self, header):\r\n        """Publish current pose estimate"""\r\n        pose_msg = PoseStamped()\r\n        pose_msg.header = header\r\n        pose_msg.header.frame_id = \'map\'\r\n\r\n        # Convert transformation matrix to pose\r\n        pose_msg.pose.position.x = self.current_pose[0, 3]\r\n        pose_msg.pose.position.y = self.current_pose[1, 3]\r\n        pose_msg.pose.position.z = self.current_pose[2, 3]\r\n\r\n        # Convert rotation matrix to quaternion\r\n        rotation_matrix = self.current_pose[:3, :3]\r\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(rotation_matrix)\r\n        pose_msg.pose.orientation.w = qw\r\n        pose_msg.pose.orientation.x = qx\r\n        pose_msg.pose.orientation.y = qy\r\n        pose_msg.pose.orientation.z = qz\r\n\r\n        self.pose_pub.publish(pose_msg)\r\n\r\n    def rotation_matrix_to_quaternion(self, R):\r\n        """Convert rotation matrix to quaternion"""\r\n        trace = np.trace(R)\r\n        if trace > 0:\r\n            s = np.sqrt(trace + 1.0) * 2\r\n            qw = 0.25 * s\r\n            qx = (R[2, 1] - R[1, 2]) / s\r\n            qy = (R[0, 2] - R[2, 0]) / s\r\n            qz = (R[1, 0] - R[0, 1]) / s\r\n        else:\r\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\r\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\r\n                qw = (R[2, 1] - R[1, 2]) / s\r\n                qx = 0.25 * s\r\n                qy = (R[0, 1] + R[1, 0]) / s\r\n                qz = (R[0, 2] + R[2, 0]) / s\r\n            elif R[1, 1] > R[2, 2]:\r\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\r\n                qw = (R[0, 2] - R[2, 0]) / s\r\n                qx = (R[0, 1] + R[1, 0]) / s\r\n                qy = 0.25 * s\r\n                qz = (R[1, 2] + R[2, 1]) / s\r\n            else:\r\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\r\n                qw = (R[1, 0] - R[0, 1]) / s\r\n                qx = (R[0, 2] + R[2, 0]) / s\r\n                qy = (R[1, 2] + R[2, 1]) / s\r\n                qz = 0.25 * s\r\n\r\n        # Normalize quaternion\r\n        norm = np.sqrt(qw*qw + qx*qx + qy*qy + qz*qz)\r\n        return qw/norm, qx/norm, qy/norm, qz/norm\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    vslam_node = IsaacVSLAMNode()\r\n    rclpy.spin(vslam_node)\r\n    vslam_node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives-review",children:"Learning Objectives Review"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the NVIDIA Isaac platform architecture and components \u2713"}),"\n",(0,s.jsx)(n.li,{children:"Set up and configure Isaac Sim for photorealistic simulation \u2713"}),"\n",(0,s.jsx)(n.li,{children:"Generate synthetic training data for robotics applications \u2713"}),"\n",(0,s.jsx)(n.li,{children:"Implement visual SLAM using Isaac ROS VSLAM \u2713"}),"\n",(0,s.jsx)(n.li,{children:"Apply Nav2 for path planning in humanoid robots \u2713"}),"\n",(0,s.jsx)(n.li,{children:"Deploy AI models from simulation to real robots using sim-to-real transfer \u2713"}),"\n",(0,s.jsx)(n.li,{children:"Build perception pipelines for autonomous robotic systems \u2713"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,s.jsx)(n.p,{children:"Install the NVIDIA Isaac ROS packages and run the basic visual SLAM example to familiarize yourself with the platform capabilities."}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the main components of the NVIDIA Isaac ecosystem?"}),"\n",(0,s.jsx)(n.li,{children:"Explain the concept of sim-to-real transfer in robotics."}),"\n",(0,s.jsx)(n.li,{children:"How does visual SLAM differ from traditional SLAM approaches?"}),"\n",(0,s.jsx)(n.li,{children:"What are the advantages of using synthetic data for robotics training?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["NVIDIA Isaac Documentation: ",(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/",children:"https://docs.nvidia.com/isaac/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Isaac ROS Documentation: ",(0,s.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/",children:"https://nvidia-isaac-ros.github.io/"})]}),"\n",(0,s.jsxs)(n.li,{children:["ROS 2 Navigation: ",(0,s.jsx)(n.a,{href:"https://navigation.ros.org/",children:"https://navigation.ros.org/"})]}),"\n",(0,s.jsx)(n.li,{children:'Computer Vision for Robotics: "Robot Vision" by Horn'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["Continue to ",(0,s.jsx)(n.a,{href:"/physical-ai/docs/module-3/nvidia-isaac-intro",children:"NVIDIA Isaac Introduction"})," to learn about the platform's architecture and setup procedures."]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>o});var r=i(6540);const s={},a=r.createContext(s);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);